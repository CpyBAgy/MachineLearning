{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Лабораторная 4 - 10 баллов (дедлайн 18.04.2025)\n",
    "- Взять код с практики и переделать его под задачу Breakout https://gymnasium.farama.org/environments/atari/breakout/#breakout .\n",
    "- Обучить модель, использовать разные оптимизации (препроцессинг стейтов и другие).\n",
    "- Визуализировать лоссы и сколько очков выбивает модель на игре при обучении, также показать графики оценки состояний во время игры (+ скриншоты из игры для каких-нибудь локальных пиков оценки состояний). Показать видео-запуск модели в онлайне.\n",
    "- Советую начинать заранее, потому что вероятно придется потратить время на обучение.\n",
    "- Если ваша модель набирает 30 очков за одну жизнь - это в целом уже успех.\n",
    "- Сделать вывод.\n"
   ],
   "id": "921df0a61af07c29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Приколы:\n",
    "На маке вообще не получилось это сделать из за конфликта версий. Дома на винде все работает, поэтому вывод скопирован с консоли просто."
   ],
   "id": "502de2f32edd4ebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Использованные в работе нормализации:\n",
    "## 1. Препроцессинг кадров\n",
    "- Перевод в оттенки серого\n",
    "- Изменение размера до 84x84\n",
    "- Нормализация (деление на 255)\n",
    "```python\n",
    "def preprocess_frame(frame):\n",
    "    ...\n",
    "```\n",
    "\n",
    "## 2. Стекинг кадров\n",
    "- Объединяет 4 последовательных кадра в один тензор.\n",
    "- Дает агенту информацию о динамике и скорости (направлении движения мяча и платформы), что невозможно определить по одному кадру.\n",
    "```python\n",
    "class FrameStack:\n",
    "    ...\n",
    "```\n",
    "\n",
    "## 3. Буфер опыта\n",
    "- Сохраняет опыт агента (state, action, reward, next_state, done) в буфере и случайным образом извлекает батчи для обучения.\n",
    "- Позволяет многократно использовать важный опыт (например, редкие события с высокой наградой).\n",
    "```python\n",
    "class ReplayBuffer:\n",
    "    ...\n",
    "```\n",
    "\n",
    "## 4. Нормализация награды\n",
    "- Приводит награды к стандартному распределению с нулевым средним и единичной дисперсией.\n",
    "- Стабилизирует обучение, делает масштаб градиентов более предсказуемым\n",
    "- Начинаем нормализацию после накопления некоторой статистики\n",
    "```python\n",
    "class RewardNormalizer:\n",
    "    ...\n",
    "```\n",
    "\n",
    "## 5. Целевая сеть (Target Network)\n",
    "- Используется для стабилизации обучения.\n",
    "- Создает копию основной сети, которая периодически обновляется копированием весов из основной сети.\n",
    "- Стабилизирует обучение, предотвращая \"погоню за собственным хвостом\" - ситуацию, когда целевые Q-значения постоянно меняются из-за обновлений основной сети.\n",
    "```python\n",
    "class DQNAgent:\n",
    "    ...\n",
    "    def update_target_network(self):\n",
    "        ...\n",
    "```\n",
    "\n",
    "## 6. Динамическе изменение Learning Rate\n",
    "- Постепенно уменьшает скорость обучения по мере прогресса тренировки.\n",
    "- Позволяет делать крупные шаги в начале обучения и более точные - к концу, когда модель приближается к оптимальному решению.\n",
    "```python\n",
    "class DQNAgent:\n",
    "    ...\n",
    "    def adjust_learning_rate(self, episode):\n",
    "        ...\n",
    "```\n",
    "\n",
    "## 7. Динамическое изменение epsilon\n",
    "- Адаптивно регулирует скорость уменьшения epsilon (вероятности случайного действия) в зависимости от прогресса обучения.\n",
    "- Позволяет модели исследовать пространство действий в начале обучения и использовать изученные действия в конце.\n",
    "- Балансирует исследование и использование. При хороших результатах быстрее переходит к использованию модели, при плохих - дольше исследует.\n",
    "```python\n",
    "class DQNAgent:\n",
    "    ...\n",
    "    def dynamic_epsilon_decay(self, best_reward):\n",
    "        ...\n",
    "```\n",
    "\n",
    "## 8. Штрафование модели за потерю жизни\n",
    "- Добавляет отрицательное вознаграждение (-1) при потере жизни в игре.\n",
    "- Явно сигнализирует агенту, что потеря жизни - нежелательное событие, что помогает выработать более осторожную стратегию игры.\n",
    "```python\n",
    "def train_agent(agent, num_episodes=2000, max_steps=10000,\n",
    "            learn_start=5000, log_interval=10, render_interval=50,\n",
    "            checkpoint_dir=\"checkpoints\"):\n",
    "    ...\n",
    "    if is_breakout and 'lives' in info:\n",
    "        current_lives = info['lives']\n",
    "        if current_lives < lives:\n",
    "            ...\n",
    "            reward -= 1\n",
    "    ...\n",
    "```\n"
   ],
   "id": "8893bfca068e99c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №0 - Импорт библиотек",
   "id": "d4fdafea9f5dc460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import traceback\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import warnings\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import logging"
   ],
   "id": "413536396e078b44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №1 - Настройка окружения",
   "id": "115f8482a185ca77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Подавление предупреждений\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger('moviepy').setLevel(logging.ERROR)\n",
    "\n",
    "# Проверка доступности GPU\n",
    "print(\"TensorFlow версия:\", tf.__version__)\n",
    "print(\"Доступны следующие GPU устройства:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Использование GPU для обучения\")\n",
    "else:\n",
    "    print(\"Использование CPU для обучения\")"
   ],
   "id": "a8f7d953e31ddbe1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №2 - Обработка фреймов",
   "id": "bb3211b0da71e3d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_frame(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    normalized = resized / 255.0\n",
    "    return normalized\n",
    "\n",
    "\n",
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Объединяет 4 последовательных кадра в один тензор.\n",
    "    Дает агенту информацию о динамике и скорости (направлении движения мяча и платформы), что невозможно определить по одному кадру.\n",
    "    \"\"\"\n",
    "    def __init__(self, stack_size=4):\n",
    "        self.stack_size = stack_size\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "\n",
    "    def reset(self, frame):\n",
    "        self.frames.clear()\n",
    "        processed = preprocess_frame(frame)\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(processed)\n",
    "        return self.get_state()\n",
    "\n",
    "    def add_frame(self, frame):\n",
    "        processed = preprocess_frame(frame)\n",
    "        self.frames.append(processed)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        frames_array = np.array(self.frames)\n",
    "        return np.moveaxis(frames_array, 0, -1)  # [frames, height, width] -> [height, width, frames] так надо тензорфлоу"
   ],
   "id": "45fc226f1687a236"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №3 - Оптимизации",
   "id": "4bea8247701cd007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Сохраняет опыт агента (state, action, reward, next_state, done) в буфере и случайным образом извлекает батчи для обучения.\n",
    "    Позволяет многократно использовать важный опыт (например, редкие события с высокой наградой).\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        states = np.array([experience[0] for experience in batch])\n",
    "        actions = np.array([experience[1] for experience in batch])\n",
    "        rewards = np.array([experience[2] for experience in batch])\n",
    "        next_states = np.array([experience[3] for experience in batch])\n",
    "        dones = np.array([experience[4] for experience in batch])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class RewardNormalizer:\n",
    "    \"\"\"\n",
    "    Приводит награды к стандартному распределению с нулевым средним и единичной дисперсией.\n",
    "    Стабилизирует обучение, делает масштаб градиентов более предсказуемым\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha  # коэффициент скользящего среднего\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.count = 0\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        self.count += 1\n",
    "        self.mean = self.mean + self.alpha * (reward - self.mean)\n",
    "        self.std = self.std + self.alpha * (((reward - self.mean) ** 2) - self.std)\n",
    "\n",
    "        norm_std = max(np.sqrt(self.std), 1e-4)\n",
    "\n",
    "        if self.count > 100:  # Начинаем нормализацию после накопления некоторой статистики\n",
    "            normalized_reward = (reward - self.mean) / norm_std\n",
    "        else:\n",
    "            normalized_reward = reward\n",
    "\n",
    "        return normalized_reward"
   ],
   "id": "28a60e144b2c69e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №4 - Создание модели",
   "id": "bcbab3dd11aae6ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_dqn_model(input_shape, n_actions, learning_rate=0.00025):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Сверточные слои\n",
    "    x = layers.Conv2D(32, 8, strides=4, activation='relu')(inputs)\n",
    "    x = layers.Conv2D(64, 4, strides=2, activation='relu')(x)\n",
    "    x = layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Полносвязные слои\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    outputs = layers.Dense(n_actions, activation='linear')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse')\n",
    "\n",
    "    return model"
   ],
   "id": "e3f33537091907a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №5 - Создание агента",
   "id": "60453c33c1396596"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions, replay_buffer_size=50000,\n",
    "                 batch_size=32, gamma=0.99, epsilon=1.0,\n",
    "                 epsilon_min=0.1, epsilon_decay=0.995,\n",
    "                 learning_rate=0.00025, update_target_epochs=5):\n",
    "\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma  # дисконтирующий фактор\n",
    "        self.epsilon = epsilon  # фактор исследования\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_epochs = update_target_epochs\n",
    "        self.steps = 0\n",
    "        self.episodes = 0\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "\n",
    "        # Создание основной модели и целевой модели\n",
    "        self.model = create_dqn_model(state_shape, n_actions, learning_rate)\n",
    "        self.target_model = create_dqn_model(state_shape, n_actions, learning_rate)\n",
    "        self.update_target_network()\n",
    "\n",
    "        self.buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "        # Для сохранения статистики\n",
    "        self.losses = []\n",
    "        self.q_values = []\n",
    "\n",
    "        # Для TensorBoard\n",
    "        self.log_dir = f\"logs/dqn_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        self.tensorboard_callback = TensorBoard(log_dir=self.log_dir,\n",
    "                                                histogram_freq=1,\n",
    "                                                update_freq=100,\n",
    "                                                write_graph=True)\n",
    "        self.summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Копирует веса из основной модели в целевую\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Выбор действия с использованием epsilon-greedy стратегии\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "\n",
    "        state_tensor = tf.convert_to_tensor(state[np.newaxis, ...], dtype=tf.float32)\n",
    "        q_values = self.model(state_tensor)\n",
    "\n",
    "        max_q = tf.reduce_max(q_values).numpy()\n",
    "        self.q_values.append(max_q)\n",
    "\n",
    "        return tf.argmax(q_values[0]).numpy()\n",
    "\n",
    "    def adjust_learning_rate(self, episode, total_episodes):\n",
    "        \"\"\"Адаптивная настройка learning rate в зависимости от прогресса обучения\"\"\"\n",
    "        progress = min(1.0, episode / (total_episodes * 0.8))\n",
    "        new_lr = self.initial_learning_rate * (1.0 - 0.9 * progress)\n",
    "\n",
    "        if self.learning_rate != new_lr:\n",
    "            self.learning_rate = new_lr\n",
    "            self.model.optimizer.lr.assign(new_lr)\n",
    "\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('learning_rate', new_lr, step=episode)\n",
    "\n",
    "    def dynamic_epsilon_decay(self, best_reward):\n",
    "        \"\"\"Динамический decay для epsilon в зависимости от прогресса обучения\"\"\"\n",
    "        if best_reward >= 20:\n",
    "            decay_factor = 0.99\n",
    "        elif best_reward >= 10:\n",
    "            decay_factor = 0.992\n",
    "        else:\n",
    "            decay_factor = self.epsilon_decay\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * decay_factor)\n",
    "\n",
    "    def learn(self, normalize_rewards=True):\n",
    "        \"\"\"Обучение модели на батче из буфера опыта\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        if normalize_rewards:\n",
    "            normalized_rewards = np.array([self.reward_normalizer.normalize(r) for r in rewards])\n",
    "        else:\n",
    "            normalized_rewards = rewards\n",
    "\n",
    "        # Приведение к тензорам TensorFlow\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(normalized_rewards, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        # Вычисление целевых Q-значений\n",
    "        next_q_values = self.target_model(next_states)\n",
    "        max_next_q = tf.reduce_max(next_q_values, axis=1)\n",
    "        target_q_values = rewards + self.gamma * max_next_q * (1 - dones)\n",
    "\n",
    "        masks = tf.one_hot(actions, self.n_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states)\n",
    "\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "\n",
    "            loss = keras.losses.Huber()(target_q_values, q_action)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        grads = [tf.clip_by_value(grad, -1.0, 1.0) for grad in grads]\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        loss_value = loss.numpy()\n",
    "        self.losses.append(loss_value)\n",
    "\n",
    "        # Логирование в TensorBoard\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss_value, step=self.steps)\n",
    "            tf.summary.scalar('max_q_value', tf.reduce_max(q_values).numpy(), step=self.steps)\n",
    "            tf.summary.scalar('mean_q_value', tf.reduce_mean(q_values).numpy(), step=self.steps)\n",
    "            tf.summary.scalar('epsilon', self.epsilon, step=self.steps)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "    def on_episode_end(self, episode, best_reward, total_episodes):\n",
    "        \"\"\"Вызывается в конце каждого эпизода для обновления параметров\"\"\"\n",
    "        self.episodes += 1\n",
    "\n",
    "        self.dynamic_epsilon_decay(best_reward)\n",
    "\n",
    "        self.adjust_learning_rate(episode, total_episodes)\n",
    "\n",
    "        if self.episodes % self.update_target_epochs == 0:\n",
    "            self.update_target_network()\n",
    "            # print(f\"Target network updated (episode {episode})\")\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Сохранение моделей и состояния агента\"\"\"\n",
    "        self.model.save(path + \"_model\")\n",
    "        self.target_model.save(path + \"_target_model\")\n",
    "\n",
    "        # Сохранение состояния агента\n",
    "        state = {\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps': self.steps,\n",
    "            'episodes': self.episodes,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'reward_normalizer_mean': self.reward_normalizer.mean,\n",
    "            'reward_normalizer_std': self.reward_normalizer.std,\n",
    "            'reward_normalizer_count': self.reward_normalizer.count\n",
    "        }\n",
    "        np.save(path + \"_state.npy\", state)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Загрузка моделей и состояния агента\"\"\"\n",
    "        self.model = keras.models.load_model(path + \"_model\")\n",
    "        self.target_model = keras.models.load_model(path + \"_target_model\")\n",
    "\n",
    "        state = np.load(path + \"_state.npy\", allow_pickle=True).item()\n",
    "        self.epsilon = state['epsilon']\n",
    "        self.steps = state['steps']\n",
    "        self.episodes = state.get('episodes', 0)\n",
    "        self.learning_rate = state.get('learning_rate', self.initial_learning_rate)\n",
    "\n",
    "        if 'reward_normalizer_mean' in state:\n",
    "            self.reward_normalizer.mean = state['reward_normalizer_mean']\n",
    "            self.reward_normalizer.std = state['reward_normalizer_std']\n",
    "            self.reward_normalizer.count = state['reward_normalizer_count']"
   ],
   "id": "bdc52e8a70c4a2be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №6 - Обучение агента",
   "id": "b32ac1591a8611af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_agent(agent, num_episodes=2000, max_steps=10000,\n",
    "                learn_start=5000, log_interval=10, render_interval=50,\n",
    "                checkpoint_dir=\"checkpoints\"):\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "    frame_stack = FrameStack(4)\n",
    "\n",
    "    best_single_life_reward = 0  # Лучший счет за одну жизнь\n",
    "    current_life_reward = 0  # Текущий счет за текущую жизнь\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    episode_rewards = []\n",
    "    best_reward = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    video_dir = \"videos\"\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    screenshot_dir = \"screenshots\"\n",
    "    os.makedirs(screenshot_dir, exist_ok=True)\n",
    "\n",
    "    total_steps = 0\n",
    "\n",
    "    print(f\"Обучение на окружении: {env.spec.id}\")\n",
    "\n",
    "    for episode in tqdm(range(1, num_episodes + 1), desc=\"Обучение\"):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        stacked_state = frame_stack.reset(state)\n",
    "        lives = 5  # В начале игры у нас 5 жизней\n",
    "\n",
    "        episode_reward = 0\n",
    "        frames_episode = []\n",
    "\n",
    "        q_values_episode = []\n",
    "\n",
    "        for step in range(1, max_steps + 1):\n",
    "            action = agent.act(stacked_state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            current_frame = env.render()\n",
    "            if current_frame is not None:\n",
    "                frames_episode.append(current_frame)\n",
    "                if len(frames_episode) > 1000:\n",
    "                    frames_episode = frames_episode[-1000:]\n",
    "\n",
    "\n",
    "            # Проверка потери жизни\n",
    "            if 'lives' in info:\n",
    "                current_lives = info['lives']\n",
    "                if current_lives < lives:\n",
    "                    if current_life_reward > best_single_life_reward:\n",
    "                        old_best_single = best_single_life_reward\n",
    "                        best_single_life_reward = current_life_reward\n",
    "                        # print(f\"Новый рекорд за одну жизнь: {best_single_life_reward} (было {old_best_single})\")\n",
    "\n",
    "                        if len(frames_episode) > 0:\n",
    "                            img = frames_episode[-1]\n",
    "                            plt.imsave(f\"{screenshot_dir}/best_single_life_{int(best_single_life_reward)}.png\", img)\n",
    "\n",
    "                            try:\n",
    "                                video_path = f\"{video_dir}/best_single_life_{int(best_single_life_reward)}.mp4\"\n",
    "                                clip = ImageSequenceClip(frames_episode, fps=30)\n",
    "                                clip.write_videofile(video_path, codec=\"libx264\", verbose=False, logger=None)\n",
    "                                # print(f\"Сохранено видео с новым рекордом за одну жизнь: {best_single_life_reward}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Ошибка при сохранении видео: {e}\")\n",
    "\n",
    "                    current_life_reward = 0\n",
    "                    lives = current_lives\n",
    "                    reward -= 1\n",
    "\n",
    "            next_stacked_state = frame_stack.add_frame(next_state)\n",
    "\n",
    "            agent.buffer.add(stacked_state, action, reward, next_stacked_state,\n",
    "                             float(terminated or truncated))\n",
    "\n",
    "            if total_steps > learn_start:\n",
    "                if total_steps > learn_start and total_steps % 5 == 0:\n",
    "                    agent.learn()\n",
    "\n",
    "            if len(agent.q_values) > 0:\n",
    "                q_values_episode.append(agent.q_values[-1])\n",
    "\n",
    "            stacked_state = next_stacked_state\n",
    "            if reward > 0:\n",
    "                current_life_reward += reward\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        if current_life_reward > best_single_life_reward:\n",
    "            old_best_single = best_single_life_reward\n",
    "            best_single_life_reward = current_life_reward\n",
    "            # print(f\"Новый рекорд за одну жизнь: {best_single_life_reward} (было {old_best_single})\")\n",
    "\n",
    "            if len(frames_episode) > 0:\n",
    "                img = frames_episode[-1]\n",
    "                plt.imsave(f\"{screenshot_dir}/best_single_life_{int(best_single_life_reward)}.png\", img)\n",
    "\n",
    "                try:\n",
    "                    video_path = f\"{video_dir}/best_single_life_{int(best_single_life_reward)}.mp4\"\n",
    "                    clip = ImageSequenceClip(frames_episode, fps=30)\n",
    "                    clip.write_videofile(video_path, codec=\"libx264\", verbose=False, logger=None)\n",
    "                    # print(f\"Сохранено видео с новым рекордом за одну жизнь: {best_single_life_reward}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при сохранении видео: {e}\")\n",
    "\n",
    "        # Запись статистики\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "        if episode_reward > best_reward:\n",
    "            old_best = best_reward\n",
    "            best_reward = episode_reward\n",
    "            # print(f\"Новый общий рекорд: {best_reward} (было {old_best})\")\n",
    "\n",
    "            if frames_episode:\n",
    "                try:\n",
    "                    model_path = f\"{checkpoint_dir}/breakout_best_{int(best_reward)}\"\n",
    "                    agent.save(model_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"DEBUG: Ошибка при сохранении модели: {e}\")\n",
    "                    import tracebac\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                try:\n",
    "                    img = frames_episode[-1]\n",
    "                    screenshot_path = f\"{screenshot_dir}/best_reward_{int(best_reward)}.png\"\n",
    "                    plt.imsave(screenshot_path, img)\n",
    "                except Exception as e:\n",
    "                    print(f\"DEBUG: Ошибка при сохранении скриншота: {e}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                try:\n",
    "                    video_path = f\"{video_dir}/best_reward_{int(best_reward)}.mp4\"\n",
    "                    clip = ImageSequenceClip(frames_episode, fps=30)\n",
    "                    clip.write_videofile(video_path, codec=\"libx264\", verbose=False, logger=None)\n",
    "                    # print(f\"Сохранено видео с новым общим рекордом: {best_reward}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"DEBUG: Ошибка при сохранении видео: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                print(f\"DEBUG: frames_episode пустой! Нет кадров для сохранения.\")\n",
    "\n",
    "        if episode % render_interval == 0 and frames_episode:\n",
    "            try:\n",
    "                video_path = f\"{video_dir}/episode_{episode}_reward_{episode_reward:.0f}.mp4\"\n",
    "                clip = ImageSequenceClip(frames_episode, fps=30)\n",
    "                clip.write_videofile(video_path, codec=\"libx264\", verbose=False, logger=None)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при сохранении видео: {e}\")\n",
    "\n",
    "        avg_reward = np.mean(episode_rewards[-min(len(episode_rewards), 100):])\n",
    "        agent.on_episode_end(episode, best_reward, num_episodes)\n",
    "\n",
    "        # Обновление TensorBoard для текущего эпизода\n",
    "        with agent.summary_writer.as_default():\n",
    "            tf.summary.scalar('episode_reward', episode_reward, step=episode)\n",
    "            tf.summary.scalar('best_reward', best_reward, step=episode)\n",
    "            tf.summary.scalar('avg_reward_100ep', avg_reward, step=episode)\n",
    "\n",
    "        if episode % 250 == 0:\n",
    "            agent.save(f\"{checkpoint_dir}/breakout_periodic_ep{episode}\")\n",
    "            # print(f\"Сохранена периодическая модель на эпизоде {episode}\")\n",
    "\n",
    "        # Вывод статистики\n",
    "        if episode % log_interval == 0:\n",
    "            avg_loss = np.mean(agent.losses[-100:]) if agent.losses else 0\n",
    "            avg_q = np.mean(agent.q_values[-100:]) if agent.q_values else 0\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            print(f\"Episode {episode}/{num_episodes} | \" +\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | Best: {best_reward:.2f} | \" +\n",
    "                  f\"Best Single Life: {best_single_life_reward:.2f} | \" +\n",
    "                  f\"Epsilon: {agent.epsilon:.4f} | Avg Loss: {avg_loss:.4f} | \" +\n",
    "                  f\"Avg Q: {avg_q:.4f} | Time: {elapsed_time:.0f}s\")\n",
    "\n",
    "    # Сохранение финальной модели\n",
    "    agent.save(f\"{checkpoint_dir}/breakout_final\")\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'losses': agent.losses,\n",
    "        'q_values': agent.q_values\n",
    "    }"
   ],
   "id": "c137cd3b409f2a2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №7 - Тестирование агента",
   "id": "15cf2e57cee69577"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_agent(agent, model_path, num_episodes=10, record_video=True):\n",
    "    agent.load(model_path)\n",
    "    agent.epsilon = 0.05  # Небольшой epsilon для небольшого исследования\n",
    "\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "    frame_stack = FrameStack(4)\n",
    "\n",
    "    is_breakout = \"Breakout\" in env.spec.id\n",
    "\n",
    "    test_rewards = []\n",
    "    os.makedirs(\"test_videos\", exist_ok=True)\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        stacked_state = frame_stack.reset(state)\n",
    "\n",
    "        episode_reward = 0\n",
    "        frames = []\n",
    "        q_values_episode = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(stacked_state, training=False)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            frames.append(env.render())\n",
    "\n",
    "            next_stacked_state = frame_stack.add_frame(next_state)\n",
    "\n",
    "            if len(agent.q_values) > 0:\n",
    "                q_values_episode.append(agent.q_values[-1])\n",
    "\n",
    "            stacked_state = next_stacked_state\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "        test_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode}: Reward = {episode_reward}\")\n",
    "\n",
    "        if record_video and frames:\n",
    "            try:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                video_path = f\"test_videos/breakout_test_{timestamp}_ep{episode}_reward{int(episode_reward)}.mp4\"\n",
    "                os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
    "                clip = ImageSequenceClip(frames, fps=30)\n",
    "                clip.write_videofile(video_path, codec=\"libx264\", verbose=False, logger=None)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при сохранении видео: {e}\")\n",
    "\n",
    "        if q_values_episode:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(q_values_episode)\n",
    "            plt.title(f\"Q-values during Test Episode {episode}\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.ylabel(\"Max Q-value\")\n",
    "            plt.savefig(f\"q_values_test_episode_{episode}.png\")\n",
    "            plt.close()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = np.mean(test_rewards)\n",
    "    print(f\"Average Test Reward: {avg_reward}\")\n",
    "    return test_rewards"
   ],
   "id": "33cfbaf6054dd455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №8 - Визуализация результатов",
   "id": "fc9fbca6cec0ffef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_training_results(results, save_dir=\"plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # График наград\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    rewards = results['episode_rewards']\n",
    "    plt.plot(rewards)\n",
    "    plt.plot(np.convolve(rewards, np.ones(10) / 10, mode='valid'), 'r--')\n",
    "    plt.title('Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{save_dir}/rewards.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # График потерь\n",
    "    if results['losses']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        losses = results['losses']\n",
    "        plt.plot(losses)\n",
    "        plt.plot(np.convolve(losses, np.ones(100) / 100, mode='valid'), 'r--')\n",
    "        plt.title('Loss during Training')\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{save_dir}/losses.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # График Q-значений\n",
    "    if results['q_values']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        q_values = results['q_values']\n",
    "        plt.plot(q_values)\n",
    "        plt.plot(np.convolve(q_values, np.ones(100) / 100, mode='valid'), 'r--')\n",
    "        plt.title('Max Q-values during Training')\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Max Q-value')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{save_dir}/q_values.png\")\n",
    "        plt.close()"
   ],
   "id": "dc9ec2d788d23cdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №9 - Запуск",
   "id": "37c253c010b78201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    try:\n",
    "        env = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "        n_actions = env.action_space.n\n",
    "        print(f\"Действий доступно: {n_actions}\")\n",
    "\n",
    "        state_shape = (84, 84, 4)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        agent = DQNAgent(state_shape, n_actions,\n",
    "                         replay_buffer_size=20000,\n",
    "                         update_target_epochs=5,\n",
    "                         epsilon_decay=0.995)\n",
    "\n",
    "        print(\"Начало обучения...\")\n",
    "        results = train_agent(\n",
    "            agent,\n",
    "            num_episodes=4001,\n",
    "            max_steps=10000,\n",
    "            learn_start=5000,\n",
    "            log_interval=10,\n",
    "            render_interval=100,\n",
    "        )\n",
    "\n",
    "        plot_training_results(results)\n",
    "\n",
    "        print(\"\\nЗадание успешно выполнено!\")\n",
    "        print(\"Результаты:\")\n",
    "        print(\"1. Графики сохранены в директории 'plots/'\")\n",
    "        print(\"2. Скриншоты состояний с высокими Q-значениями в 'screenshots/'\")\n",
    "        print(\"3. Видео обучения в 'videos/' и тестирования в 'test_videos/'\")\n",
    "        print(\"4. Логи TensorBoard сохранены в директории 'logs/'\")\n",
    "        print(\"\\nДля просмотра логов в TensorBoard выполните:\")\n",
    "        print(\"tensorboard --logdir=logs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "58f432d4846de6e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Шаг 10 - Тестирование и анализ Q-значений\n",
    "## Для этого воспользоваться файлом `testing.py`"
   ],
   "id": "ea6cb4b7d04e8304"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Выводы и анализ графиков\n",
    "\n",
    "# Анализ обучения DQN-агента в игре Breakout\n",
    "\n",
    "## Введение\n",
    "\n",
    "В рамках данной лабораторной работы была реализована глубокая Q-сеть (Deep Q-Network, DQN) для обучения агента игре в Atari Breakout. Задача заключалась в переработке кода с практики под конкретную игровую среду, применении различных оптимизаций и анализе результатов обучения.\n",
    "\n",
    "## Результаты результаты обучения\n",
    "\n",
    "- **Средняя награда за последние 100 эпизодов**: 48.50 очков\n",
    "- **Лучший результат за все время**: 318.00 очков\n",
    "- **Лучший результат за одну жизнь**: 277.00 очков\n",
    "- **Текущее значение эпсилон**: 0.1000\n",
    "- **Средняя функция потерь**: 0.0926\n",
    "- **Среднее Q-значение**: 2.0411\n",
    "\n",
    "Эти метрики значительно превосходят целевой показатель задания в 30 очков за одну жизнь, что свидетельствует об успешной реализации и оптимизации алгоритма DQN для данной игровой среды.\n",
    "\n",
    "## Анализ графиков обучения\n",
    "\n",
    "### 1. Средняя награда за 100 эпизодов\n",
    "\n",
    "![Средняя награда за 100 эпизодов](plots/avg_reward_100ep.png)\n",
    "\n",
    "График средней награды демонстрирует стабильный прогресс обучения агента. Начиная с отрицательных значений в первых эпизодах (что типично для случайных действий на начальном этапе), к эпизоду 500 агент достиг положительных результатов. Особенно заметен прогресс после 1000-го эпизода, когда кривая начала устойчиво расти, достигнув около 30 очков в среднем к эпизоду 2000.\n",
    "\n",
    "Сглаженная кривая (красная пунктирная линия) наглядно демонстрирует устойчивую тенденцию к улучшению. Примечательно, что к эпизоду 2000 агент не достиг плато в обучении, что указывало на потенциал дальнейшего улучшения — и действительно, к эпизоду 4000 средняя награда выросла до 48.50 очков.\n",
    "\n",
    "### 2. Лучшая достигнутая награда\n",
    "\n",
    "![Лучшая достигнутая награда](plots/best_reward.png)\n",
    "\n",
    "График лучшей награды показывает максимальные достижения агента на разных этапах обучения. Ступенчатая структура графика отражает моменты, когда агент достигал новых рекордов производительности. К эпизоду 2000 лучший результат составлял около 78 очков, однако при продолжении обучения эта метрика значительно улучшилась, достигнув впечатляющих 318.00 очков.\n",
    "\n",
    "### 3. Награда за эпизод\n",
    "\n",
    "![Награда за эпизод](plots/episode_reward.png)\n",
    "\n",
    "График награды за эпизод демонстрирует фактическую производительность агента в каждом отдельном эпизоде. Характерна высокая волатильность результатов, что типично для сложных сред с элементами случайности. Несмотря на колебания, красная линия скользящего среднего показывает устойчивый рост средней производительности.\n",
    "\n",
    "Важно отметить, что в поздней фазе обучения (после 1500 эпизодов) агент несколько раз достигал результатов выше 70-80 очков, а при дальнейшем обучении смог достичь результатов выше 300 очков, что является исключительным достижением.\n",
    "\n",
    "### 4. Значение эпсилон\n",
    "\n",
    "![Значение эпсилон](plots/epsilon.png)\n",
    "\n",
    "График показывает динамику параметра ε (эпсилон), который контролирует баланс между исследованием и эксплуатацией в стратегии ε-жадного выбора действий. Параметр быстро уменьшается на ранних этапах обучения (примерно до 20000 шагов), после чего достигает минимального значения 0.1.\n",
    "\n",
    "Такая стратегия decay эпсилона оправдана: в начале обучения агент больше исследует среду, а по мере накопления опыта всё больше полагается на выученную стратегию. Минимальное значение 0.1 поддерживает некоторую степень исследования даже на поздних этапах, что помогает избежать застревания в локальных оптимумах.\n",
    "\n",
    "### 5. Скорость обучения\n",
    "\n",
    "![Скорость обучения](plots/learning_rate.png)\n",
    "\n",
    "График демонстрирует линейное снижение скорости обучения (learning rate) на протяжении всего процесса тренировки. Начальное значение около 0.00025 постепенно снижается до примерно 0.00011 к 2000-му эпизоду. \n",
    "\n",
    "Этот адаптивный подход к настройке скорости обучения способствует стабильности тренировки: более высокие значения в начале позволяют быстрее обучаться, а более низкие значения на поздних этапах обеспечивают тонкую настройку модели и предотвращают перескакивание через оптимум.\n",
    "\n",
    "### 6. Функция потерь\n",
    "\n",
    "![Функция потерь](plots/loss.png)\n",
    "\n",
    "График функции потерь показывает высокую волатильность с тенденцией к снижению. Несмотря на значительные колебания отдельных значений (синяя линия), скользящее среднее (красная линия) демонстрирует общее снижение и стабилизацию ошибки.\n",
    "\n",
    "### 7. Максимальные Q-значения\n",
    "\n",
    "![Максимальные Q-значения](plots/max_q_value.png)\n",
    "\n",
    "График максимальных Q-значений демонстрирует интересную динамику. В начале обучения (первые 50000 шагов) наблюдается быстрый рост Q-значений, после чего следует снижение и последующая стабилизация с постепенным повышением.\n",
    "\n",
    "Максимальные Q-значения достигают пиков около 17.5, что указывает на формирование агентом оптимистичных оценок для определенных состояний и действий. Локальные пики на графике соответствуют ситуациям, которые модель оценивает как особенно перспективные (например, момент перед разбиванием блока или удачное позиционирование платформы).\n",
    "\n",
    "### 8. Средние Q-значения\n",
    "\n",
    "![Средние Q-значения](plots/mean_q_value.png)\n",
    "\n",
    "График средних Q-значений следует схожей тенденции с максимальными Q-значениями, но с меньшей амплитудой. После первоначального роста и последующего снижения, значения стабилизируются в диапазоне 3-4. К эпизоду 3290 среднее Q-значение составляет 2.0411.\n",
    "\n",
    "Такая динамика характерна для успешно обучающейся DQN и свидетельствует о том, что агент формирует более реалистичные и сбалансированные оценки ценности различных состояний и действий.\n",
    "\n",
    "## Применённые оптимизации\n",
    "\n",
    "В ходе работы были применены следующие оптимизации, которые значительно повысили эффективность обучения:\n",
    "\n",
    "1. **Эффективный препроцессинг состояний**:\n",
    "   - Использование класса `FrameStack` для хранения 4 последовательных кадров\n",
    "   - Преобразование кадров в оттенки серого\n",
    "   - Изменение размера до 84x84 пикселей\n",
    "   - Нормализация значений пикселей\n",
    "\n",
    "2. **Адаптивные гиперпараметры**:\n",
    "   - Динамический decay для epsilon в методе `dynamic_epsilon_decay`\n",
    "   - Адаптивная настройка скорости обучения в методе `adjust_learning_rate`\n",
    "\n",
    "3. **Стабилизация обучения**:\n",
    "   - Нормализация вознаграждений с помощью класса `RewardNormalizer`\n",
    "   - Ограничение градиентов для предотвращения взрывов\n",
    "   - Использование функции потерь Huber вместо MSE для большей устойчивости\n",
    "\n",
    "4. **Архитектура сети**:\n",
    "   - Использование сверточной нейронной сети с 3 сверточными слоями\n",
    "   - Полносвязный слой с 512 нейронами\n",
    "   - Линейный выходной слой для Q-значений\n",
    "\n",
    "5. **Механизмы обновления целевой сети**:\n",
    "   - Периодическое обновление target-сети каждые 5 эпизодов\n",
    "   - Использование отдельных основной и целевой сетей\n",
    "\n",
    "6. **Система вознаграждений**:\n",
    "   - Штраф за потерю жизни (-1)\n",
    "   - Отслеживание лучшего результата за одну жизнь\n",
    "\n",
    "## Анализ состояний с высокими Q-значениями\n",
    "\n",
    "Для более глубокого понимания процесса обучения DQN-агента важно проанализировать, какие именно состояния игры модель оценивает как наиболее перспективные (имеющие высокие Q-значения). График максимальных Q-значений показывает несколько характерных пиков, достигающих значений около 17-18, что существенно выше среднего уровня.\n",
    "\n",
    "Анализируя сохраненные скриншоты и соотнося их с графиком Q-значений, можно выделить несколько типичных игровых ситуаций, которые модель научилась высоко оценивать:\n",
    "\n",
    "1. **Пробивание \"туннелей\"** - когда шарик пробивает вертикальный канал в блоках и получает доступ к верхним рядам. В этих ситуациях Q-значения резко возрастают, так как модель \"понимает\", что такая позиция потенциально ведет к большому количеству очков.\n",
    "\n",
    "![Туннель](screenshots/best_single_life_219.png)\n",
    "\n",
    "2. **Отражение шарика от боковой стенки под удачным углом** - позволяет поразить труднодоступные блоки. В сохраненных скриншотах из моментов с высокими наградами часто наблюдается именно такая стратегия.\n",
    "\n",
    "![Угол](screenshots/best_reward_27.png)\n",
    "\n",
    "3**Последние блоки в ряду** - модель особенно высоко оценивает состояния, когда шарик направляется к последнему блоку в ряду, что может открыть новые возможности для набора очков.\n",
    "\n",
    "![Последний в ряду](screenshots/best_reward_71.png)\n",
    "\n",
    "Интересно отметить, что локальные пики Q-значений наблюдаются не только в моменты набора очков, но и в подготовительных фазах, когда агент создает благоприятную позицию для последующих действий. Это свидетельствует о формировании долгосрочной стратегии, а не только реактивного поведения.\n",
    "\n",
    "## Выводы\n",
    "\n",
    "Разработанный DQN-агент продемонстрировал исключительные результаты в игре Breakout, значительно превзойдя целевой показатель задания в 30 очков за одну жизнь. Достижение 277 очков за одну жизнь и 318 очков за игру свидетельствует о высокой эффективности применённых оптимизаций и адекватности выбранной архитектуры модели.\n",
    "\n",
    "Анализ графиков показывает устойчивый процесс обучения с последовательным улучшением производительности. Динамика Q-значений указывает на способность модели формировать всё более точные оценки ценности различных состояний и действий.\n",
    "\n",
    "Особенно эффективными оказались:\n",
    "- Препроцессинг и стекинг кадров, позволивший агенту отслеживать динамику движения\n",
    "- Адаптивные гиперпараметры, обеспечившие баланс между исследованием и эксплуатацией\n",
    "- Нормализация вознаграждений, стабилизировавшая процесс обучения\n",
    "\n",
    "Важной особенностью реализации стал механизм сохранения и визуализации результатов, включая сохранение видео лучших игр и моделей с наивысшими показателями.\n",
    "\n",
    "В целом, проект успешно демонстрирует применение глубокого обучения с подкреплением для решения сложных игровых задач, требующих как стратегического планирования, так и быстрых тактических решений.\n"
   ],
   "id": "8d5138d6920184bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75051e82880f04e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
