{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Лабораторная 2 - 10 баллов (дедлайн 14.03)\n",
    "## Задача классификации цветов автомобилей.\n",
    "### Датасет DVM: https://deepvisualmarketing.github.io/ фронтальный!\n",
    "## Ход работы:\n",
    "Написать своими руками классификатор любой на выбор: ResNet, InceptionV3, DenseNet, MobileNet, ShuffleNet и обучить его на полученном датасете.\n",
    "\n",
    "Также взять аналогичный классификатор, но предобученный на ImageNet или на Cityscapes и дообучить на собранном датасете. Выяснить, чей классификатор лучше.\n",
    "\n",
    "Оценка качества производится при помощи F1_macro, требуется получить F1_macro > 0.8.\n",
    "\n",
    "### Сравнить полученное качество и сделать вывод."
   ],
   "id": "caf5701529577b28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №0 - Импорт библиотек",
   "id": "52eb872d6d8205b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import compute_class_weight\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ],
   "id": "f500c91bd78b612b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №1 - Функции для загрузки и предварительной обработки данных",
   "id": "db01fe3977a40d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ensure_class_representation(train_df, val_df):\n",
    "    \"\"\"\n",
    "    Функция для проверки и обеспечения равномерного распределения классов в обучающей и валидационной выборках\n",
    "    Description: Исключает данный из обучающей выборки, если такой класс отсутствует в валидационной выборке\n",
    "    \"\"\"\n",
    "    val_classes = Counter(val_df['color'])\n",
    "    train_classes = Counter(train_df['color'])\n",
    "\n",
    "    for cls in train_classes:\n",
    "        if cls not in val_classes or val_classes[cls] == 0:\n",
    "            samples = train_df[train_df['color'] == cls].sample(1)\n",
    "            if not samples.empty:\n",
    "                val_df = pd.concat([val_df, samples])\n",
    "                train_df = train_df.drop(samples.index)\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def prepare_data(data_dir, batch_size=32, img_size=(299, 299), min_samples=500, excluded_colors=None):\n",
    "    \"\"\"\n",
    "    Функция для подготовки данных для обучения модели\n",
    "    Description: Загружает изображения из директории, разделяет их на обучающую, валидационную и тестовую выборки,\n",
    "    обеспечивает равномерное распределение классов, создает генераторы для обучения, валидации и тестирования\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    colors = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    color = file.split('$$')[3]\n",
    "                    image_paths.append(full_path)\n",
    "                    colors.append(color)\n",
    "                except IndexError:\n",
    "                    print(f\"Пропускаем файл с неправильным форматом: {file}\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'color': colors\n",
    "    })\n",
    "\n",
    "    print(f\"Распределение цветов в исходном датасете:\")\n",
    "    print(df['color'].value_counts())\n",
    "    print(f\"Количество классов: {df['color'].nunique()}\")\n",
    "\n",
    "    if excluded_colors is None:\n",
    "        excluded_colors = ['Multicolour', 'Unlisted']\n",
    "\n",
    "    df = df[~df['color'].isin(excluded_colors)]\n",
    "\n",
    "    color_counts = df['color'].value_counts()\n",
    "    valid_colors = color_counts[color_counts >= min_samples].index.tolist()\n",
    "\n",
    "    df = df[df['color'].isin(valid_colors)]\n",
    "\n",
    "    normalized_dfs = []\n",
    "    for color in valid_colors:\n",
    "        color_df = df[df['color'] == color]\n",
    "        if len(color_df) > min_samples:\n",
    "            normalized_dfs.append(color_df.sample(min_samples, random_state=42))\n",
    "        else:\n",
    "            normalized_dfs.append(color_df)\n",
    "\n",
    "    df = pd.concat(normalized_dfs)\n",
    "\n",
    "    print(f\"Распределение цветов в отфильтрованном и нормализованном датасете:\")\n",
    "    print(df['color'].value_counts())\n",
    "    print(f\"Количество классов: {df['color'].nunique()}\")\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    try:\n",
    "        train_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['color'], random_state=42)\n",
    "        train_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['color'],\n",
    "                                            random_state=42)\n",
    "    except ValueError as e:\n",
    "        print(f\"Предупреждение: {e}\")\n",
    "        print(\"Переключаемся на нестратифицированное разделение данных\")\n",
    "        train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_df, val_df = ensure_class_representation(train_df, val_df)\n",
    "\n",
    "    print(f\"Размер обучающей выборки: {len(train_df)}\")\n",
    "    print(f\"Размер валидационной выборки: {len(val_df)}\")\n",
    "    print(f\"Размер тестовой выборки: {len(test_df)}\")\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        zoom_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.9, 1.1],\n",
    "        fill_mode='nearest',\n",
    "    )\n",
    "\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='image_path',\n",
    "        y_col='color',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    validation_generator = val_test_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='image_path',\n",
    "        y_col='color',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_generator = val_test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='image_path',\n",
    "        y_col='color',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator, test_generator, train_generator.class_indices"
   ],
   "id": "1a6cabcac00c6591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №2 - Определение класса для создания модели",
   "id": "ea4d13a40b85897e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CarColorClassifier:\n",
    "    \"\"\"\n",
    "    Класс для создания модели классификатора цветов автомобилей\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(299, 299, 3), num_classes=None, useGPU=False):\n",
    "        \"\"\"\n",
    "        Функция инициализации\n",
    "        :param input_shape: размер входного изображения\n",
    "        :param num_classes: количество классов для классификации\n",
    "        :param useGPU: флаг использования GPU\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        self.useGPU = useGPU\n",
    "\n",
    "        if self.useGPU:\n",
    "            physical_devices = tf.config.list_physical_devices('GPU')\n",
    "            if len(physical_devices) > 0:\n",
    "                print(f\"Найдено {len(physical_devices)} GPU устройств. Будет использоваться GPU.\")\n",
    "                for device in physical_devices:\n",
    "                    tf.config.experimental.set_memory_growth(device, True)\n",
    "            else:\n",
    "                print(\"GPU не обнаружен, будет использоваться CPU несмотря на параметр useGPU=True.\")\n",
    "                self.useGPU = False\n",
    "        else:\n",
    "            print(\"Выбран режим CPU.\")\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "    def build_inception_from_scratch(self):\n",
    "        \"\"\"\n",
    "        Функция для создания модели InceptionV3 с нуля\n",
    "        \"\"\"\n",
    "        with tf.device('/GPU:0' if self.useGPU else '/CPU:0'):\n",
    "            inputs = keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "            x = keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='valid')(inputs)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "            x = keras.layers.Conv2D(64, (3, 3), padding='valid')(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "            x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "            for i in range(5):\n",
    "                filter_scale = min(1 + i * 0.3, 2.5)\n",
    "\n",
    "                branch1x1 = keras.layers.Conv2D(int(64 * filter_scale), (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "                branch5x5 = keras.layers.Conv2D(int(48 * filter_scale), (1, 1), padding='same', activation='relu')(x)\n",
    "                branch5x5 = keras.layers.Conv2D(int(64 * filter_scale), (5, 5), padding='same', activation='relu')(\n",
    "                    branch5x5)\n",
    "\n",
    "                branch3x3dbl = keras.layers.Conv2D(int(64 * filter_scale), (1, 1), padding='same', activation='relu')(x)\n",
    "                branch3x3dbl = keras.layers.Conv2D(int(96 * filter_scale), (3, 3), padding='same', activation='relu')(\n",
    "                    branch3x3dbl)\n",
    "                branch3x3dbl = keras.layers.Conv2D(int(96 * filter_scale), (3, 3), padding='same', activation='relu')(\n",
    "                    branch3x3dbl)\n",
    "\n",
    "                branch_pool = keras.layers.AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "                branch_pool = keras.layers.Conv2D(int(64 * filter_scale), (1, 1), padding='same', activation='relu')(\n",
    "                    branch_pool)\n",
    "\n",
    "                x = keras.layers.Concatenate(axis=-1)([branch1x1, branch5x5, branch3x3dbl, branch_pool])\n",
    "\n",
    "                x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "                if i >= 2:\n",
    "                    x = keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "                if (i + 1) % 2 == 0 and i < 4:\n",
    "                    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "            x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "            x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "            x = keras.layers.Dense(512, activation='relu')(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "            x = keras.layers.Dense(256, activation='relu')(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "            x = keras.layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "            model = keras.Model(inputs, x)\n",
    "            return model\n",
    "\n",
    "    def build_pretrained_model(self):\n",
    "        \"\"\"\n",
    "        Функция для создания предобученной модели InceptionV3\n",
    "        \"\"\"\n",
    "        with tf.device('/GPU:0' if self.useGPU else '/CPU:0'):\n",
    "            base_model = keras.applications.InceptionV3(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "\n",
    "            x = base_model.output\n",
    "            x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "            x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "            x = keras.layers.Dropout(0.5)(x)\n",
    "            x = keras.layers.Dense(512, activation='relu')(x)\n",
    "            x = keras.layers.Dropout(0.3)(x)\n",
    "            x = keras.layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "            model = keras.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            return model\n",
    "\n",
    "    def compile_model(self, learning_rate=0.0005):\n",
    "        \"\"\"\n",
    "        Функция для компиляции модели\n",
    "        \"\"\"\n",
    "        with tf.device('/GPU:0' if self.useGPU else '/CPU:0'):\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            self.model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "    def train(self, train_generator, validation_generator, epochs=50, callbacks=None, class_weight=None):\n",
    "        \"\"\"\n",
    "        Функция для обучения модели\n",
    "        \"\"\"\n",
    "        with tf.device('/GPU:0' if self.useGPU else '/CPU:0'):\n",
    "            history = self.model.fit(\n",
    "                train_generator,\n",
    "                validation_data=validation_generator,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                class_weight=class_weight,\n",
    "                verbose=1\n",
    "            )\n",
    "            return history\n",
    "\n",
    "    def evaluate(self, test_generator):\n",
    "        \"\"\"\n",
    "        Функция для оценки качества модели\n",
    "        \"\"\"\n",
    "        with tf.device('/GPU:0' if self.useGPU else '/CPU:0'):\n",
    "            return self.model.evaluate(test_generator)\n",
    "\n",
    "    def predict(self, test_generator):\n",
    "        \"\"\"\n",
    "        Функция для предсказания классов\n",
    "        \"\"\"\n",
    "        with tf.device('/GPU:0' if self.useGPU else '/CPU:0'):\n",
    "            return self.model.predict(test_generator)"
   ],
   "id": "a8545cd12a7513b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №3 - Определение класса для вычисления F1 макро",
   "id": "a6e1e070c951937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class F1Callback(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Класс для вычисления F1 макро на валидационной выборке\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data):\n",
    "        \"\"\"\n",
    "        Функция инициализации\n",
    "        :param validation_data: данные для валидации\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Функция для вычисления F1 макро на валидационной выборке\n",
    "        \"\"\"\n",
    "        x_val, y_val = self.validation_data\n",
    "        y_pred = np.argmax(self.model.predict(x_val), axis=1)\n",
    "        y_true = np.argmax(y_val, axis=1)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        print(f' - val_f1_macro: {f1:.4f}')\n",
    "        logs['val_f1_macro'] = f1"
   ],
   "id": "d590fff2615668e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №4 - Определение функций для визуализации результатов",
   "id": "9c520f46030d7d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_generator, class_indices):\n",
    "    \"\"\"\n",
    "    Функция для оценки качества модели и построения матрицы ошибок\n",
    "    \"\"\"\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    class_labels = {v: k for k, v in class_indices.items()}\n",
    "    target_names = [class_labels[i] for i in range(len(class_labels))]\n",
    "\n",
    "    print(\"\\nКлассификационный отчет:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "    print(f\"\\nF1 Macro: {f1_macro:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Матрица ошибок')\n",
    "    plt.ylabel('Истинный класс')\n",
    "    plt.xlabel('Предсказанный класс')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return f1_macro\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Функция для визуализации истории обучения модели и построения графиков\n",
    "    \"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d1237c0666790b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №5 - Определение вспомогательной функции для валидации скорости обучения в зависимости от количества эпох",
   "id": "2cb3cb2f94e70a63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lr_schedule(epoch, initial_lr=0.0005):\n",
    "    \"\"\"\n",
    "    Функция для планирования скорости обучения\n",
    "    \"\"\"\n",
    "    lr = initial_lr\n",
    "    if epoch > 30:\n",
    "        lr *= 0.1\n",
    "    elif epoch > 20:\n",
    "        lr *= 0.5\n",
    "    return lr"
   ],
   "id": "ab0eb0dda2a81700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №6 - Определение поведения (эксперимента) кастомной модели",
   "id": "fc5da6047112e083"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_experiment_custom(data_dir, batch_size=32, img_size=(299, 299), epochs=30, learning_rate=0.0005, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Функция для запуска эксперимента с кастомной моделью\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Эксперимент с моделью InceptionV3 с нуля ===\")\n",
    "    print(f\"Режим обучения: {'GPU' if use_gpu else 'CPU'}\")\n",
    "\n",
    "    train_generator, validation_generator, test_generator, class_indices = prepare_data(\n",
    "        data_dir, batch_size, img_size\n",
    "    )\n",
    "\n",
    "    num_classes = len(class_indices)\n",
    "    classifier = CarColorClassifier(input_shape=(img_size[0], img_size[1], 3), num_classes=num_classes, useGPU=use_gpu)\n",
    "    classifier.model = classifier.build_inception_from_scratch()\n",
    "    classifier.compile_model(learning_rate=learning_rate)\n",
    "\n",
    "    val_data = next(validation_generator)\n",
    "    f1_callback = F1Callback(val_data)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_custom_model.keras', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        f1_callback\n",
    "    ]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_generator.classes),\n",
    "        y=train_generator.classes\n",
    "    )\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    history = classifier.train(\n",
    "        train_generator,\n",
    "        validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "    plot_training_history(history)\n",
    "\n",
    "    f1_macro = evaluate_model(classifier.model, test_generator, class_indices)\n",
    "\n",
    "    return classifier.model, f1_macro"
   ],
   "id": "9bd55d6f41d8383d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №7 - Определение поведения (эксперимента) предобученной модели",
   "id": "1d10c1f44b26df57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_experiment_pretrained(data_dir, batch_size=32, img_size=(299, 299), epochs=30, learning_rate=0.0005, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Функция для запуска эксперимента с предобученной моделью\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Эксперимент с предобученной моделью InceptionV3 ===\")\n",
    "    print(f\"Режим обучения: {'GPU' if use_gpu else 'CPU'}\")\n",
    "\n",
    "    train_generator, validation_generator, test_generator, class_indices = prepare_data(\n",
    "        data_dir, batch_size, img_size\n",
    "    )\n",
    "\n",
    "    num_classes = len(class_indices)\n",
    "    classifier = CarColorClassifier(input_shape=(img_size[0], img_size[1], 3), num_classes=num_classes, useGPU=use_gpu)\n",
    "    classifier.model = classifier.build_pretrained_model()\n",
    "    classifier.compile_model(learning_rate=learning_rate)\n",
    "\n",
    "    val_data = next(validation_generator)\n",
    "    f1_callback = F1Callback(val_data)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_generator.classes),\n",
    "        y=train_generator.classes\n",
    "    )\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    history = classifier.train(\n",
    "        train_generator,\n",
    "        validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "    plot_training_history(history)\n",
    "\n",
    "    print(\"\\nДообучение с разблокировкой последних слоев...\")\n",
    "    for layer in classifier.model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    classifier.compile_model(learning_rate=learning_rate / 10)\n",
    "\n",
    "    history_fine = classifier.train(\n",
    "        train_generator,\n",
    "        validation_generator,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "    plot_training_history(history_fine)\n",
    "\n",
    "    f1_macro = evaluate_model(classifier.model, test_generator, class_indices)\n",
    "\n",
    "    return classifier.model, f1_macro"
   ],
   "id": "9d519c82a16c8da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №8 - Определение начальных параметров",
   "id": "f0d96c73f5d22640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"dataset\"\n",
    "\n",
    "batch_size = 32\n",
    "img_size = (299, 299)\n",
    "epochs = 100\n",
    "learning_rate = 0.00005\n",
    "use_gpu = True"
   ],
   "id": "8ce07c69e198375f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №9 - Запуск экспериментов",
   "id": "20600cb30d257301"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\n=== Начало эксперимента ===\")\n",
    "print(f\"Использование GPU: {use_gpu}\")\n",
    "\n",
    "if use_gpu:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        print(f\"Доступно GPU устройств: {len(physical_devices)}\")\n",
    "        print(f\"Устройства: {physical_devices}\")\n",
    "    else:\n",
    "        print(\"GPU не обнаружен. Рекомендуется установить параметр use_gpu=False.\")\n",
    "\n",
    "custom_model, custom_f1 = run_experiment_custom(\n",
    "    data_dir, batch_size, img_size, epochs, learning_rate, use_gpu\n",
    ")\n",
    "\n",
    "pretrained_model, pretrained_f1 = run_experiment_pretrained(\n",
    "    data_dir, batch_size, img_size, epochs, learning_rate, use_gpu\n",
    ")"
   ],
   "id": "fac94f5e70368349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Шаг №10 - Сравнение результатов",
   "id": "72a9873cb12bb2bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Сравнение результатов ===\")\n",
    "print(f\"F1 Macro модели с нуля: {custom_f1:.4f}\")\n",
    "print(f\"F1 Macro предобученной модели: {pretrained_f1:.4f}\")\n",
    "\n",
    "if pretrained_f1 >= custom_f1:\n",
    "    print(\"Предобученная модель показала лучшие результаты.\")\n",
    "else:\n",
    "    print(\"Модель, обученная с нуля, показала лучшие результаты.\")"
   ],
   "id": "393f82f53f14094b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Вывод для первого эксперимента\n",
    "Вывод скопировал из консоли домашнего компьютера, тк там не запустился jupiter notebook, а на ноутбуке было грустно из-за отсутствия видеокарты\n"
   ],
   "id": "3fea44c7c336ed47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "=== Эксперимент с моделью InceptionV3 с нуля ===\n",
    "Режим обучения: GPU\n",
    "Распределение цветов в исходном датасете:\n",
    "color\n",
    "Black          14317\n",
    "Grey            9474\n",
    "White           9395\n",
    "Blue            8483\n",
    "Silver          7770\n",
    "Red             6095\n",
    "Unlisted        1516\n",
    "Brown            911\n",
    "Green            777\n",
    "Yellow           667\n",
    "Beige            600\n",
    "Orange           559\n",
    "Purple           362\n",
    "Bronze           329\n",
    "Gold             217\n",
    "Multicolour      196\n",
    "Pink              87\n",
    "Turquoise         26\n",
    "Maroon            26\n",
    "Burgundy           9\n",
    "Magenta            9\n",
    "Navy               1\n",
    "Indigo             1\n",
    "Name: count, dtype: int64\n",
    "Количество классов: 23\n",
    "Распределение цветов в отфильтрованном и нормализованном датасете:\n",
    "color\n",
    "Black     500\n",
    "Grey      500\n",
    "White     500\n",
    "Blue      500\n",
    "Silver    500\n",
    "Red       500\n",
    "Brown     500\n",
    "Green     500\n",
    "Yellow    500\n",
    "Beige     500\n",
    "Orange    500\n",
    "Name: count, dtype: int64\n",
    "Количество классов: 11\n",
    "Размер обучающей выборки: 3520\n",
    "Размер валидационной выборки: 880\n",
    "Размер тестовой выборки: 1100\n",
    "Found 3520 validated image filenames belonging to 11 classes.\n",
    "Found 880 validated image filenames belonging to 11 classes.\n",
    "Found 1100 validated image filenames belonging to 11 classes.\n",
    "Найдено 1 GPU устройств. Будет использоваться GPU.\n",
    "Epoch 1/100\n",
    "  6/110 [>.............................] - ETA: 30s - loss: 3.0701 - accuracy: 0.1042WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1468s). Check your callbacks.\n",
    "110/110 [==============================] - ETA: 0s - loss: 2.3827 - accuracy: 0.2665\n",
    "Epoch 1: val_loss improved from inf to 2.50598, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 380ms/step\n",
    " - val_f1_macro: 0.0076\n",
    "110/110 [==============================] - 60s 461ms/step - loss: 2.3827 - accuracy: 0.2665 - val_loss: 2.5060 - val_accuracy: 0.0352 - lr: 5.0000e-05 - val_f1_macro: 0.0076\n",
    "Epoch 2/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.9502 - accuracy: 0.3724\n",
    "Epoch 2: val_loss did not improve from 2.50598\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.0214\n",
    "110/110 [==============================] - 54s 488ms/step - loss: 1.9502 - accuracy: 0.3724 - val_loss: 2.5824 - val_accuracy: 0.0898 - lr: 5.0000e-05 - val_f1_macro: 0.0214\n",
    "Epoch 3/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.7580 - accuracy: 0.4168\n",
    "Epoch 3: val_loss did not improve from 2.50598\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.0375\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 1.7580 - accuracy: 0.4168 - val_loss: 2.6778 - val_accuracy: 0.0977 - lr: 5.0000e-05 - val_f1_macro: 0.0375\n",
    "Epoch 4/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.6219 - accuracy: 0.4639\n",
    "Epoch 4: val_loss did not improve from 2.50598\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.1564\n",
    "110/110 [==============================] - 54s 490ms/step - loss: 1.6219 - accuracy: 0.4639 - val_loss: 2.5068 - val_accuracy: 0.1807 - lr: 5.0000e-05 - val_f1_macro: 0.1564\n",
    "Epoch 5/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.5780 - accuracy: 0.4807\n",
    "Epoch 5: val_loss improved from 2.50598 to 1.55190, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.2901\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 1.5780 - accuracy: 0.4807 - val_loss: 1.5519 - val_accuracy: 0.4045 - lr: 5.0000e-05 - val_f1_macro: 0.2901\n",
    "Epoch 6/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.4558 - accuracy: 0.5060\n",
    "Epoch 6: val_loss improved from 1.55190 to 1.38036, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.3230\n",
    "110/110 [==============================] - 56s 504ms/step - loss: 1.4558 - accuracy: 0.5060 - val_loss: 1.3804 - val_accuracy: 0.5000 - lr: 5.0000e-05 - val_f1_macro: 0.3230\n",
    "Epoch 7/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.4213 - accuracy: 0.5122\n",
    "Epoch 7: val_loss improved from 1.38036 to 1.32663, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 26ms/step\n",
    " - val_f1_macro: 0.3964\n",
    "110/110 [==============================] - 55s 498ms/step - loss: 1.4213 - accuracy: 0.5122 - val_loss: 1.3266 - val_accuracy: 0.5295 - lr: 5.0000e-05 - val_f1_macro: 0.3964\n",
    "Epoch 8/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3682 - accuracy: 0.5352\n",
    "Epoch 8: val_loss did not improve from 1.32663\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.3384\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.3682 - accuracy: 0.5352 - val_loss: 1.5625 - val_accuracy: 0.4716 - lr: 5.0000e-05 - val_f1_macro: 0.3384\n",
    "Epoch 9/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3207 - accuracy: 0.5500\n",
    "Epoch 9: val_loss improved from 1.32663 to 1.26143, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.3325\n",
    "110/110 [==============================] - 55s 500ms/step - loss: 1.3207 - accuracy: 0.5500 - val_loss: 1.2614 - val_accuracy: 0.5580 - lr: 5.0000e-05 - val_f1_macro: 0.3325\n",
    "Epoch 10/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3340 - accuracy: 0.5543\n",
    "Epoch 10: val_loss did not improve from 1.26143\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4333\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.3340 - accuracy: 0.5543 - val_loss: 1.3434 - val_accuracy: 0.5307 - lr: 5.0000e-05 - val_f1_macro: 0.4333\n",
    "Epoch 11/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2746 - accuracy: 0.5685\n",
    "Epoch 11: val_loss improved from 1.26143 to 1.15693, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.5207\n",
    "110/110 [==============================] - 56s 505ms/step - loss: 1.2746 - accuracy: 0.5685 - val_loss: 1.1569 - val_accuracy: 0.6193 - lr: 5.0000e-05 - val_f1_macro: 0.5207\n",
    "Epoch 12/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2171 - accuracy: 0.5827\n",
    "Epoch 12: val_loss did not improve from 1.15693\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4116\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.2171 - accuracy: 0.5827 - val_loss: 1.2626 - val_accuracy: 0.5659 - lr: 5.0000e-05 - val_f1_macro: 0.4116\n",
    "Epoch 13/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1745 - accuracy: 0.5835\n",
    "Epoch 13: val_loss did not improve from 1.15693\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.5428\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.1745 - accuracy: 0.5835 - val_loss: 1.2440 - val_accuracy: 0.5966 - lr: 5.0000e-05 - val_f1_macro: 0.5428\n",
    "Epoch 14/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1374 - accuracy: 0.6057\n",
    "Epoch 14: val_loss did not improve from 1.15693\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.3994\n",
    "110/110 [==============================] - 54s 488ms/step - loss: 1.1374 - accuracy: 0.6057 - val_loss: 1.1735 - val_accuracy: 0.5739 - lr: 5.0000e-05 - val_f1_macro: 0.3994\n",
    "Epoch 15/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1154 - accuracy: 0.6139\n",
    "Epoch 15: val_loss did not improve from 1.15693\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4167\n",
    "110/110 [==============================] - 54s 490ms/step - loss: 1.1154 - accuracy: 0.6139 - val_loss: 1.5063 - val_accuracy: 0.4670 - lr: 5.0000e-05 - val_f1_macro: 0.4167\n",
    "Epoch 16/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0820 - accuracy: 0.6239\n",
    "Epoch 16: val_loss improved from 1.15693 to 1.15474, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.5100\n",
    "110/110 [==============================] - 55s 502ms/step - loss: 1.0820 - accuracy: 0.6239 - val_loss: 1.1547 - val_accuracy: 0.5886 - lr: 5.0000e-05 - val_f1_macro: 0.5100\n",
    "Epoch 17/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0740 - accuracy: 0.6244\n",
    "Epoch 17: val_loss did not improve from 1.15474\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4224\n",
    "110/110 [==============================] - 54s 491ms/step - loss: 1.0740 - accuracy: 0.6244 - val_loss: 1.1935 - val_accuracy: 0.5750 - lr: 5.0000e-05 - val_f1_macro: 0.4224\n",
    "Epoch 18/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0634 - accuracy: 0.6315\n",
    "Epoch 18: val_loss did not improve from 1.15474\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.3743\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 1.0634 - accuracy: 0.6315 - val_loss: 1.3696 - val_accuracy: 0.5534 - lr: 5.0000e-05 - val_f1_macro: 0.3743\n",
    "Epoch 19/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0010 - accuracy: 0.6474\n",
    "Epoch 19: val_loss did not improve from 1.15474\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4128\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 1.0010 - accuracy: 0.6474 - val_loss: 1.2375 - val_accuracy: 0.5841 - lr: 5.0000e-05 - val_f1_macro: 0.4128\n",
    "Epoch 20/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.6554\n",
    "Epoch 20: val_loss did not improve from 1.15474\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4529\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.9954 - accuracy: 0.6554 - val_loss: 1.3582 - val_accuracy: 0.5886 - lr: 5.0000e-05 - val_f1_macro: 0.4529\n",
    "Epoch 21/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.9486 - accuracy: 0.6682\n",
    "Epoch 21: val_loss improved from 1.15474 to 1.14608, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.5745\n",
    "110/110 [==============================] - 56s 505ms/step - loss: 0.9486 - accuracy: 0.6682 - val_loss: 1.1461 - val_accuracy: 0.6045 - lr: 5.0000e-05 - val_f1_macro: 0.5745\n",
    "Epoch 22/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.9158 - accuracy: 0.6733\n",
    "Epoch 22: val_loss did not improve from 1.14608\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.3960\n",
    "110/110 [==============================] - 54s 491ms/step - loss: 0.9158 - accuracy: 0.6733 - val_loss: 1.4417 - val_accuracy: 0.5670 - lr: 2.5000e-05 - val_f1_macro: 0.3960\n",
    "Epoch 23/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8663 - accuracy: 0.7011\n",
    "Epoch 23: val_loss improved from 1.14608 to 1.00274, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4466\n",
    "110/110 [==============================] - 56s 505ms/step - loss: 0.8663 - accuracy: 0.7011 - val_loss: 1.0027 - val_accuracy: 0.6648 - lr: 1.2500e-05 - val_f1_macro: 0.4466\n",
    "Epoch 24/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8361 - accuracy: 0.7054\n",
    "Epoch 24: val_loss improved from 1.00274 to 0.83762, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4977\n",
    "110/110 [==============================] - 56s 504ms/step - loss: 0.8361 - accuracy: 0.7054 - val_loss: 0.8376 - val_accuracy: 0.6989 - lr: 6.2500e-06 - val_f1_macro: 0.4977\n",
    "Epoch 25/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.7136\n",
    "Epoch 25: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.4812\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.8291 - accuracy: 0.7136 - val_loss: 0.8560 - val_accuracy: 0.6989 - lr: 3.1250e-06 - val_f1_macro: 0.4812\n",
    "Epoch 26/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.7091\n",
    "Epoch 26: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 491ms/step - loss: 0.8303 - accuracy: 0.7091 - val_loss: 0.8456 - val_accuracy: 0.7057 - lr: 1.5625e-06 - val_f1_macro: 0.4462\n",
    "Epoch 27/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.7125\n",
    "Epoch 27: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 0.8172 - accuracy: 0.7125 - val_loss: 0.8415 - val_accuracy: 0.7080 - lr: 7.8125e-07 - val_f1_macro: 0.4462\n",
    "Epoch 28/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.7159\n",
    "Epoch 28: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 32ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 0.8093 - accuracy: 0.7159 - val_loss: 0.8409 - val_accuracy: 0.7045 - lr: 3.9062e-07 - val_f1_macro: 0.4462\n",
    "Epoch 29/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8431 - accuracy: 0.7068\n",
    "Epoch 29: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.8431 - accuracy: 0.7068 - val_loss: 0.8405 - val_accuracy: 0.7034 - lr: 1.9531e-07 - val_f1_macro: 0.4462\n",
    "Epoch 30/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8224 - accuracy: 0.7006\n",
    "Epoch 30: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 0.8224 - accuracy: 0.7006 - val_loss: 0.8442 - val_accuracy: 0.7045 - lr: 9.7656e-08 - val_f1_macro: 0.4462\n",
    "Epoch 31/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8129 - accuracy: 0.7134\n",
    "Epoch 31: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 498ms/step - loss: 0.8129 - accuracy: 0.7134 - val_loss: 0.8430 - val_accuracy: 0.7034 - lr: 4.8828e-08 - val_f1_macro: 0.4462\n",
    "Epoch 32/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.7145\n",
    "Epoch 32: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 500ms/step - loss: 0.8074 - accuracy: 0.7145 - val_loss: 0.8397 - val_accuracy: 0.7045 - lr: 4.8828e-09 - val_f1_macro: 0.4462\n",
    "Epoch 33/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.7097\n",
    "Epoch 33: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.8172 - accuracy: 0.7097 - val_loss: 0.8435 - val_accuracy: 0.7034 - lr: 4.8828e-10 - val_f1_macro: 0.4462\n",
    "Epoch 34/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8111 - accuracy: 0.7111\n",
    "Epoch 34: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 499ms/step - loss: 0.8111 - accuracy: 0.7111 - val_loss: 0.8440 - val_accuracy: 0.7045 - lr: 4.8828e-11 - val_f1_macro: 0.4462\n",
    "Epoch 35/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8271 - accuracy: 0.7131\n",
    "Epoch 35: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 498ms/step - loss: 0.8271 - accuracy: 0.7131 - val_loss: 0.8423 - val_accuracy: 0.7023 - lr: 4.8828e-12 - val_f1_macro: 0.4462\n",
    "Epoch 36/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8440 - accuracy: 0.7014\n",
    "Epoch 36: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 491ms/step - loss: 0.8440 - accuracy: 0.7014 - val_loss: 0.8441 - val_accuracy: 0.7034 - lr: 4.8828e-13 - val_f1_macro: 0.4462\n",
    "Epoch 37/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.7179\n",
    "Epoch 37: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 0.7992 - accuracy: 0.7179 - val_loss: 0.8424 - val_accuracy: 0.7045 - lr: 4.8828e-14 - val_f1_macro: 0.4462\n",
    "Epoch 38/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8236 - accuracy: 0.7142\n",
    "Epoch 38: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 0.8236 - accuracy: 0.7142 - val_loss: 0.8425 - val_accuracy: 0.7034 - lr: 4.8828e-15 - val_f1_macro: 0.4462\n",
    "Epoch 39/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8129 - accuracy: 0.7085\n",
    "Epoch 39: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 0.8129 - accuracy: 0.7085 - val_loss: 0.8446 - val_accuracy: 0.7034 - lr: 4.8828e-16 - val_f1_macro: 0.4462\n",
    "Epoch 40/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8047 - accuracy: 0.7162\n",
    "Epoch 40: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 0.8047 - accuracy: 0.7162 - val_loss: 0.8442 - val_accuracy: 0.7034 - lr: 4.8828e-17 - val_f1_macro: 0.4462\n",
    "Epoch 41/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.7995 - accuracy: 0.7185\n",
    "Epoch 41: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.7995 - accuracy: 0.7185 - val_loss: 0.8445 - val_accuracy: 0.7023 - lr: 4.8828e-18 - val_f1_macro: 0.4462\n",
    "Epoch 42/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.7145\n",
    "Epoch 42: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.8040 - accuracy: 0.7145 - val_loss: 0.8447 - val_accuracy: 0.7011 - lr: 4.8828e-19 - val_f1_macro: 0.4462\n",
    "Epoch 43/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8306 - accuracy: 0.7071\n",
    "Epoch 43: val_loss did not improve from 0.83762\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 0.8306 - accuracy: 0.7071 - val_loss: 0.8403 - val_accuracy: 0.7034 - lr: 4.8828e-20 - val_f1_macro: 0.4462\n",
    "Epoch 44/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.7082\n",
    "Epoch 44: val_loss improved from 0.83762 to 0.83745, saving model to best_custom_model.keras\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 56s 508ms/step - loss: 0.8231 - accuracy: 0.7082 - val_loss: 0.8375 - val_accuracy: 0.7034 - lr: 4.8828e-21 - val_f1_macro: 0.4462\n",
    "Epoch 45/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8209 - accuracy: 0.7099\n",
    "Epoch 45: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 32ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 0.8209 - accuracy: 0.7099 - val_loss: 0.8424 - val_accuracy: 0.7023 - lr: 4.8828e-22 - val_f1_macro: 0.4462\n",
    "Epoch 46/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.7065\n",
    "Epoch 46: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 0.8280 - accuracy: 0.7065 - val_loss: 0.8433 - val_accuracy: 0.7057 - lr: 4.8828e-23 - val_f1_macro: 0.4462\n",
    "Epoch 47/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8224 - accuracy: 0.7094\n",
    "Epoch 47: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 493ms/step - loss: 0.8224 - accuracy: 0.7094 - val_loss: 0.8425 - val_accuracy: 0.7057 - lr: 4.8828e-24 - val_f1_macro: 0.4462\n",
    "Epoch 48/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8321 - accuracy: 0.7034\n",
    "Epoch 48: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 0.8321 - accuracy: 0.7034 - val_loss: 0.8424 - val_accuracy: 0.7023 - lr: 4.8828e-25 - val_f1_macro: 0.4462\n",
    "Epoch 49/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8102 - accuracy: 0.7139\n",
    "Epoch 49: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 0.8102 - accuracy: 0.7139 - val_loss: 0.8419 - val_accuracy: 0.7045 - lr: 4.8828e-26 - val_f1_macro: 0.4462\n",
    "Epoch 50/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8184 - accuracy: 0.7145\n",
    "Epoch 50: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 0.8184 - accuracy: 0.7145 - val_loss: 0.8445 - val_accuracy: 0.7045 - lr: 4.8828e-27 - val_f1_macro: 0.4462\n",
    "Epoch 51/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8076 - accuracy: 0.7227\n",
    "Epoch 51: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 500ms/step - loss: 0.8076 - accuracy: 0.7227 - val_loss: 0.8419 - val_accuracy: 0.7034 - lr: 4.8828e-28 - val_f1_macro: 0.4462\n",
    "Epoch 52/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8269 - accuracy: 0.7077\n",
    "Epoch 52: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 498ms/step - loss: 0.8269 - accuracy: 0.7077 - val_loss: 0.8413 - val_accuracy: 0.7034 - lr: 4.8828e-29 - val_f1_macro: 0.4462\n",
    "Epoch 53/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8206 - accuracy: 0.7134\n",
    "Epoch 53: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 0.8206 - accuracy: 0.7134 - val_loss: 0.8403 - val_accuracy: 0.7068 - lr: 4.8828e-30 - val_f1_macro: 0.4462\n",
    "Epoch 54/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8166 - accuracy: 0.7247\n",
    "Epoch 54: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 0.8166 - accuracy: 0.7247 - val_loss: 0.8414 - val_accuracy: 0.7034 - lr: 4.8828e-31 - val_f1_macro: 0.4462\n",
    "Epoch 55/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8324 - accuracy: 0.7057\n",
    "Epoch 55: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 32ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 0.8324 - accuracy: 0.7057 - val_loss: 0.8419 - val_accuracy: 0.7045 - lr: 4.8828e-32 - val_f1_macro: 0.4462\n",
    "Epoch 56/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8282 - accuracy: 0.7097\n",
    "Epoch 56: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 28ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 490ms/step - loss: 0.8282 - accuracy: 0.7097 - val_loss: 0.8430 - val_accuracy: 0.7045 - lr: 4.8828e-33 - val_f1_macro: 0.4462\n",
    "Epoch 57/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.7151\n",
    "Epoch 57: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 0.8172 - accuracy: 0.7151 - val_loss: 0.8384 - val_accuracy: 0.7057 - lr: 4.8828e-34 - val_f1_macro: 0.4462\n",
    "Epoch 58/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8134 - accuracy: 0.7071\n",
    "Epoch 58: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 0.8134 - accuracy: 0.7071 - val_loss: 0.8422 - val_accuracy: 0.7057 - lr: 4.8828e-35 - val_f1_macro: 0.4462\n",
    "Epoch 59/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8351 - accuracy: 0.7094\n",
    "Epoch 59: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 0.8351 - accuracy: 0.7094 - val_loss: 0.8383 - val_accuracy: 0.7080 - lr: 4.8828e-36 - val_f1_macro: 0.4462\n",
    "Epoch 60/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8135 - accuracy: 0.7094\n",
    "Epoch 60: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 0.8135 - accuracy: 0.7094 - val_loss: 0.8397 - val_accuracy: 0.7045 - lr: 4.8828e-37 - val_f1_macro: 0.4462\n",
    "Epoch 61/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.7173\n",
    "Epoch 61: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 0.8036 - accuracy: 0.7173 - val_loss: 0.8397 - val_accuracy: 0.7057 - lr: 4.8828e-38 - val_f1_macro: 0.4462\n",
    "Epoch 62/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8104 - accuracy: 0.7097\n",
    "Epoch 62: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 0.8104 - accuracy: 0.7097 - val_loss: 0.8424 - val_accuracy: 0.7045 - lr: 4.8828e-39 - val_f1_macro: 0.4462\n",
    "Epoch 63/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.7182\n",
    "Epoch 63: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 0.8196 - accuracy: 0.7182 - val_loss: 0.8403 - val_accuracy: 0.7034 - lr: 4.8828e-40 - val_f1_macro: 0.4462\n",
    "Epoch 64/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.7185\n",
    "Epoch 64: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 31ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 499ms/step - loss: 0.7952 - accuracy: 0.7185 - val_loss: 0.8436 - val_accuracy: 0.7034 - lr: 4.8828e-41 - val_f1_macro: 0.4462\n",
    "Epoch 65/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8394 - accuracy: 0.7068\n",
    "Epoch 65: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 30ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 490ms/step - loss: 0.8394 - accuracy: 0.7068 - val_loss: 0.8463 - val_accuracy: 0.7045 - lr: 4.8821e-42 - val_f1_macro: 0.4462\n",
    "Epoch 66/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.7051\n",
    "Epoch 66: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 491ms/step - loss: 0.8303 - accuracy: 0.7051 - val_loss: 0.8458 - val_accuracy: 0.7045 - lr: 4.8765e-43 - val_f1_macro: 0.4462\n",
    "Epoch 67/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8391 - accuracy: 0.7080\n",
    "Epoch 67: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 0.8391 - accuracy: 0.7080 - val_loss: 0.8456 - val_accuracy: 0.7045 - lr: 4.9045e-44 - val_f1_macro: 0.4462\n",
    "Epoch 68/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.7931 - accuracy: 0.7273\n",
    "Epoch 68: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 29ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 0.7931 - accuracy: 0.7273 - val_loss: 0.8419 - val_accuracy: 0.7068 - lr: 5.6052e-45 - val_f1_macro: 0.4462\n",
    "Epoch 69/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.7097Restoring model weights from the end of the best epoch: 44.\n",
    "\n",
    "Epoch 69: val_loss did not improve from 0.83745\n",
    "1/1 [==============================] - 0s 27ms/step\n",
    " - val_f1_macro: 0.4462\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 0.8193 - accuracy: 0.7097 - val_loss: 0.8429 - val_accuracy: 0.7034 - lr: 0.0000e+00 - val_f1_macro: 0.4462\n",
    "Epoch 69: early stopping\n",
    "35/35 [==============================] - 5s 143ms/step\n",
    "\n",
    "Классификационный отчет:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Beige       0.68      0.72      0.70       100\n",
    "       Black       0.48      0.70      0.57       100\n",
    "        Blue       0.70      0.74      0.72       100\n",
    "       Brown       0.64      0.58      0.61       100\n",
    "       Green       0.73      0.63      0.68       100\n",
    "        Grey       0.46      0.53      0.49       100\n",
    "      Orange       0.95      0.75      0.84       100\n",
    "         Red       0.84      0.94      0.89       100\n",
    "      Silver       0.58      0.56      0.57       100\n",
    "       White       0.95      0.63      0.76       100\n",
    "      Yellow       0.97      0.95      0.96       100\n",
    "\n",
    "    accuracy                           0.76       910\n",
    "   macro avg       0.77      0.74      0.75       910\n",
    "weighted avg       0.78      0.76      0.76       910\n",
    "\n",
    "\n",
    "F1 Macro: 0.7454\n",
    "\"\"\""
   ],
   "id": "3cbf87bd70c35dc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Итоги по первому эксперименту\n",
    "### F1 Macro: 0.7454\n",
    "![Графики обучения модели](plots/graph_custom.jpg)\n",
    "![Матрица ошибок](plots/matrix_custom.jpg)\n"
   ],
   "id": "9972cfcb4e543d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Вывод для второго эксперимента\n",
    "Вывод скопировал из консоли домашнего компьютера, тк там не запустился jupiter notebook, а на ноутбуке было грустно из-за отсутствия видеокарты\n"
   ],
   "id": "85aa9944cc6f9bf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "=== Эксперимент с предобученной моделью InceptionV3 ===\n",
    "Режим обучения: GPU\n",
    "Распределение цветов в исходном датасете:\n",
    "color\n",
    "Black          14317\n",
    "Grey            9474\n",
    "White           9395\n",
    "Blue            8483\n",
    "Silver          7770\n",
    "Red             6095\n",
    "Unlisted        1516\n",
    "Brown            911\n",
    "Green            777\n",
    "Yellow           667\n",
    "Beige            600\n",
    "Orange           559\n",
    "Purple           362\n",
    "Bronze           329\n",
    "Gold             217\n",
    "Multicolour      196\n",
    "Pink              87\n",
    "Turquoise         26\n",
    "Maroon            26\n",
    "Burgundy           9\n",
    "Magenta            9\n",
    "Navy               1\n",
    "Indigo             1\n",
    "Name: count, dtype: int64\n",
    "Количество классов: 23\n",
    "Распределение цветов в отфильтрованном и нормализованном датасете:\n",
    "color\n",
    "Black     500\n",
    "Grey      500\n",
    "White     500\n",
    "Blue      500\n",
    "Silver    500\n",
    "Red       500\n",
    "Brown     500\n",
    "Green     500\n",
    "Yellow    500\n",
    "Beige     500\n",
    "Orange    500\n",
    "Name: count, dtype: int64\n",
    "Количество классов: 11\n",
    "Размер обучающей выборки: 3520\n",
    "Размер валидационной выборки: 880\n",
    "Размер тестовой выборки: 1100\n",
    "Found 3520 validated image filenames belonging to 11 classes.\n",
    "Found 880 validated image filenames belonging to 11 classes.\n",
    "Found 1100 validated image filenames belonging to 11 classes.\n",
    "Найдено 1 GPU устройств. Будет использоваться GPU.\n",
    "Epoch 1/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 2.3727 - accuracy: 0.1489\n",
    "Epoch 1: val_loss improved from inf to 2.13398, saving model to best_model.keras\n",
    "110/110 [==============================] - 60s 499ms/step - loss: 2.3727 - accuracy: 0.1489 - val_loss: 2.1340 - val_accuracy: 0.3102 - lr: 5.0000e-05\n",
    "Epoch 2/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 2.1055 - accuracy: 0.2497\n",
    "Epoch 2: val_loss improved from 2.13398 to 1.87985, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 2.1055 - accuracy: 0.2497 - val_loss: 1.8798 - val_accuracy: 0.3739 - lr: 5.0000e-05\n",
    "Epoch 3/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.8987 - accuracy: 0.3224\n",
    "Epoch 3: val_loss improved from 1.87985 to 1.71647, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 494ms/step - loss: 1.8987 - accuracy: 0.3224 - val_loss: 1.7165 - val_accuracy: 0.4432 - lr: 5.0000e-05\n",
    "Epoch 4/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.7894 - accuracy: 0.3438\n",
    "Epoch 4: val_loss improved from 1.71647 to 1.62447, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 494ms/step - loss: 1.7894 - accuracy: 0.3438 - val_loss: 1.6245 - val_accuracy: 0.4455 - lr: 5.0000e-05\n",
    "Epoch 5/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.6949 - accuracy: 0.3793\n",
    "Epoch 5: val_loss improved from 1.62447 to 1.54841, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 1.6949 - accuracy: 0.3793 - val_loss: 1.5484 - val_accuracy: 0.4875 - lr: 5.0000e-05\n",
    "Epoch 6/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.6302 - accuracy: 0.4023\n",
    "Epoch 6: val_loss improved from 1.54841 to 1.51023, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 503ms/step - loss: 1.6302 - accuracy: 0.4023 - val_loss: 1.5102 - val_accuracy: 0.4682 - lr: 5.0000e-05\n",
    "Epoch 7/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.5634 - accuracy: 0.4227\n",
    "Epoch 7: val_loss improved from 1.51023 to 1.45108, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 1.5634 - accuracy: 0.4227 - val_loss: 1.4511 - val_accuracy: 0.4943 - lr: 5.0000e-05\n",
    "Epoch 8/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.5359 - accuracy: 0.4352\n",
    "Epoch 8: val_loss improved from 1.45108 to 1.40813, saving model to best_model.keras\n",
    "110/110 [==============================] - 56s 504ms/step - loss: 1.5359 - accuracy: 0.4352 - val_loss: 1.4081 - val_accuracy: 0.5170 - lr: 5.0000e-05\n",
    "Epoch 9/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.4929 - accuracy: 0.4517\n",
    "Epoch 9: val_loss improved from 1.40813 to 1.37302, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 495ms/step - loss: 1.4929 - accuracy: 0.4517 - val_loss: 1.3730 - val_accuracy: 0.5318 - lr: 5.0000e-05\n",
    "Epoch 10/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.4761 - accuracy: 0.4645\n",
    "Epoch 10: val_loss improved from 1.37302 to 1.34285, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 484ms/step - loss: 1.4761 - accuracy: 0.4645 - val_loss: 1.3429 - val_accuracy: 0.5455 - lr: 5.0000e-05\n",
    "Epoch 11/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.4227 - accuracy: 0.4719\n",
    "Epoch 11: val_loss improved from 1.34285 to 1.33842, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 483ms/step - loss: 1.4227 - accuracy: 0.4719 - val_loss: 1.3384 - val_accuracy: 0.5386 - lr: 5.0000e-05\n",
    "Epoch 12/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3935 - accuracy: 0.4852\n",
    "Epoch 12: val_loss improved from 1.33842 to 1.29965, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 1.3935 - accuracy: 0.4852 - val_loss: 1.2996 - val_accuracy: 0.5489 - lr: 5.0000e-05\n",
    "Epoch 13/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3864 - accuracy: 0.5014\n",
    "Epoch 13: val_loss improved from 1.29965 to 1.27937, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 1.3864 - accuracy: 0.5014 - val_loss: 1.2794 - val_accuracy: 0.5705 - lr: 5.0000e-05\n",
    "Epoch 14/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3739 - accuracy: 0.4929\n",
    "Epoch 14: val_loss did not improve from 1.27937\n",
    "110/110 [==============================] - 53s 481ms/step - loss: 1.3739 - accuracy: 0.4929 - val_loss: 1.2838 - val_accuracy: 0.5591 - lr: 5.0000e-05\n",
    "Epoch 15/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3388 - accuracy: 0.5105\n",
    "Epoch 15: val_loss improved from 1.27937 to 1.25111, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 488ms/step - loss: 1.3388 - accuracy: 0.5105 - val_loss: 1.2511 - val_accuracy: 0.5705 - lr: 5.0000e-05\n",
    "Epoch 16/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.3141 - accuracy: 0.5182\n",
    "Epoch 16: val_loss improved from 1.25111 to 1.24000, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 481ms/step - loss: 1.3141 - accuracy: 0.5182 - val_loss: 1.2400 - val_accuracy: 0.5602 - lr: 5.0000e-05\n",
    "Epoch 17/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2989 - accuracy: 0.5310\n",
    "Epoch 17: val_loss improved from 1.24000 to 1.22536, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 492ms/step - loss: 1.2989 - accuracy: 0.5310 - val_loss: 1.2254 - val_accuracy: 0.5648 - lr: 5.0000e-05\n",
    "Epoch 18/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2903 - accuracy: 0.5324\n",
    "Epoch 18: val_loss improved from 1.22536 to 1.20828, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 484ms/step - loss: 1.2903 - accuracy: 0.5324 - val_loss: 1.2083 - val_accuracy: 0.5818 - lr: 5.0000e-05\n",
    "Epoch 19/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2646 - accuracy: 0.5403\n",
    "Epoch 19: val_loss improved from 1.20828 to 1.19477, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 482ms/step - loss: 1.2646 - accuracy: 0.5403 - val_loss: 1.1948 - val_accuracy: 0.5807 - lr: 5.0000e-05\n",
    "Epoch 20/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2652 - accuracy: 0.5415\n",
    "Epoch 20: val_loss did not improve from 1.19477\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.2652 - accuracy: 0.5415 - val_loss: 1.2077 - val_accuracy: 0.5773 - lr: 5.0000e-05\n",
    "Epoch 21/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2259 - accuracy: 0.5528\n",
    "Epoch 21: val_loss improved from 1.19477 to 1.17525, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 482ms/step - loss: 1.2259 - accuracy: 0.5528 - val_loss: 1.1752 - val_accuracy: 0.5830 - lr: 5.0000e-05\n",
    "Epoch 22/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2047 - accuracy: 0.5611\n",
    "Epoch 22: val_loss improved from 1.17525 to 1.16405, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 483ms/step - loss: 1.2047 - accuracy: 0.5611 - val_loss: 1.1640 - val_accuracy: 0.5852 - lr: 2.5000e-05\n",
    "Epoch 23/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1853 - accuracy: 0.5688\n",
    "Epoch 23: val_loss improved from 1.16405 to 1.14050, saving model to best_model.keras\n",
    "110/110 [==============================] - 53s 482ms/step - loss: 1.1853 - accuracy: 0.5688 - val_loss: 1.1405 - val_accuracy: 0.6091 - lr: 1.2500e-05\n",
    "Epoch 24/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1792 - accuracy: 0.5750\n",
    "Epoch 24: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1792 - accuracy: 0.5750 - val_loss: 1.1462 - val_accuracy: 0.6000 - lr: 6.2500e-06\n",
    "Epoch 25/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1748 - accuracy: 0.5651\n",
    "Epoch 25: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1748 - accuracy: 0.5651 - val_loss: 1.1411 - val_accuracy: 0.6034 - lr: 3.1250e-06\n",
    "Epoch 26/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1662 - accuracy: 0.5815\n",
    "Epoch 26: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1662 - accuracy: 0.5815 - val_loss: 1.1410 - val_accuracy: 0.6068 - lr: 1.5625e-06\n",
    "Epoch 27/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1542 - accuracy: 0.5892\n",
    "Epoch 27: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 469ms/step - loss: 1.1542 - accuracy: 0.5892 - val_loss: 1.1408 - val_accuracy: 0.6068 - lr: 7.8125e-07\n",
    "Epoch 28/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1823 - accuracy: 0.5730\n",
    "Epoch 28: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1823 - accuracy: 0.5730 - val_loss: 1.1407 - val_accuracy: 0.6068 - lr: 3.9062e-07\n",
    "Epoch 29/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1532 - accuracy: 0.5884\n",
    "Epoch 29: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1532 - accuracy: 0.5884 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 1.9531e-07\n",
    "Epoch 30/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1709 - accuracy: 0.5707\n",
    "Epoch 30: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1709 - accuracy: 0.5707 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 9.7656e-08\n",
    "Epoch 31/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1611 - accuracy: 0.5855\n",
    "Epoch 31: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1611 - accuracy: 0.5855 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-08\n",
    "Epoch 32/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1577 - accuracy: 0.5744\n",
    "Epoch 32: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1577 - accuracy: 0.5744 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-09\n",
    "Epoch 33/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1468 - accuracy: 0.5750\n",
    "Epoch 33: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 469ms/step - loss: 1.1468 - accuracy: 0.5750 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-10\n",
    "Epoch 34/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1601 - accuracy: 0.5722\n",
    "Epoch 34: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1601 - accuracy: 0.5722 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-11\n",
    "Epoch 35/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1576 - accuracy: 0.5895\n",
    "Epoch 35: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1576 - accuracy: 0.5895 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-12\n",
    "Epoch 36/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1713 - accuracy: 0.5690\n",
    "Epoch 36: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1713 - accuracy: 0.5690 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-13\n",
    "Epoch 37/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1679 - accuracy: 0.5781\n",
    "Epoch 37: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1679 - accuracy: 0.5781 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-14\n",
    "Epoch 38/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1729 - accuracy: 0.5801\n",
    "Epoch 38: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1729 - accuracy: 0.5801 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-15\n",
    "Epoch 39/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1713 - accuracy: 0.5741\n",
    "Epoch 39: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1713 - accuracy: 0.5741 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-16\n",
    "Epoch 40/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1726 - accuracy: 0.5793\n",
    "Epoch 40: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 470ms/step - loss: 1.1726 - accuracy: 0.5793 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-17\n",
    "Epoch 41/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1702 - accuracy: 0.5770\n",
    "Epoch 41: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 472ms/step - loss: 1.1702 - accuracy: 0.5770 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-18\n",
    "Epoch 42/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1700 - accuracy: 0.5750\n",
    "Epoch 42: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 471ms/step - loss: 1.1700 - accuracy: 0.5750 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-19\n",
    "Epoch 43/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1763 - accuracy: 0.5736Restoring model weights from the end of the best epoch: 23.\n",
    "\n",
    "Epoch 43: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 52s 472ms/step - loss: 1.1763 - accuracy: 0.5736 - val_loss: 1.1406 - val_accuracy: 0.6068 - lr: 4.8828e-20\n",
    "Epoch 43: early stopping\n",
    "\n",
    "Дообучение с разблокировкой последних слоев...\n",
    "Epoch 1/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2504 - accuracy: 0.5384\n",
    "Epoch 1: val_loss did not improve from 1.14050\n",
    "110/110 [==============================] - 57s 487ms/step - loss: 1.2504 - accuracy: 0.5384 - val_loss: 1.1454 - val_accuracy: 0.6034 - lr: 5.0000e-06\n",
    "Epoch 2/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.2074 - accuracy: 0.5639\n",
    "Epoch 2: val_loss improved from 1.14050 to 1.13691, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 1.2074 - accuracy: 0.5639 - val_loss: 1.1369 - val_accuracy: 0.6000 - lr: 5.0000e-06\n",
    "Epoch 3/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1969 - accuracy: 0.5653\n",
    "Epoch 3: val_loss improved from 1.13691 to 1.13140, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 491ms/step - loss: 1.1969 - accuracy: 0.5653 - val_loss: 1.1314 - val_accuracy: 0.6091 - lr: 5.0000e-06\n",
    "Epoch 4/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1840 - accuracy: 0.5753\n",
    "Epoch 4: val_loss improved from 1.13140 to 1.12736, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 1.1840 - accuracy: 0.5753 - val_loss: 1.1274 - val_accuracy: 0.6114 - lr: 5.0000e-06\n",
    "Epoch 5/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1584 - accuracy: 0.5787\n",
    "Epoch 5: val_loss improved from 1.12736 to 1.12064, saving model to best_model.keras\n",
    "110/110 [==============================] - 68s 617ms/step - loss: 1.1584 - accuracy: 0.5787 - val_loss: 1.1206 - val_accuracy: 0.6216 - lr: 5.0000e-06\n",
    "Epoch 6/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1644 - accuracy: 0.5838\n",
    "Epoch 6: val_loss improved from 1.12064 to 1.11523, saving model to best_model.keras\n",
    "110/110 [==============================] - 64s 581ms/step - loss: 1.1644 - accuracy: 0.5838 - val_loss: 1.1152 - val_accuracy: 0.6205 - lr: 5.0000e-06\n",
    "Epoch 7/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1677 - accuracy: 0.5753\n",
    "Epoch 7: val_loss improved from 1.11523 to 1.11453, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 1.1677 - accuracy: 0.5753 - val_loss: 1.1145 - val_accuracy: 0.6125 - lr: 5.0000e-06\n",
    "Epoch 8/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1351 - accuracy: 0.5849\n",
    "Epoch 8: val_loss improved from 1.11453 to 1.10784, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 493ms/step - loss: 1.1351 - accuracy: 0.5849 - val_loss: 1.1078 - val_accuracy: 0.6205 - lr: 5.0000e-06\n",
    "Epoch 9/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1424 - accuracy: 0.5875\n",
    "Epoch 9: val_loss improved from 1.10784 to 1.10109, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 488ms/step - loss: 1.1424 - accuracy: 0.5875 - val_loss: 1.1011 - val_accuracy: 0.6239 - lr: 5.0000e-06\n",
    "Epoch 10/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1386 - accuracy: 0.5872\n",
    "Epoch 10: val_loss did not improve from 1.10109\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.1386 - accuracy: 0.5872 - val_loss: 1.1020 - val_accuracy: 0.6227 - lr: 5.0000e-06\n",
    "Epoch 11/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1093 - accuracy: 0.5980\n",
    "Epoch 11: val_loss improved from 1.10109 to 1.09721, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 494ms/step - loss: 1.1093 - accuracy: 0.5980 - val_loss: 1.0972 - val_accuracy: 0.6216 - lr: 5.0000e-06\n",
    "Epoch 12/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1269 - accuracy: 0.5983\n",
    "Epoch 12: val_loss improved from 1.09721 to 1.09321, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 486ms/step - loss: 1.1269 - accuracy: 0.5983 - val_loss: 1.0932 - val_accuracy: 0.6250 - lr: 5.0000e-06\n",
    "Epoch 13/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1249 - accuracy: 0.5858\n",
    "Epoch 13: val_loss improved from 1.09321 to 1.09287, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.1249 - accuracy: 0.5858 - val_loss: 1.0929 - val_accuracy: 0.6148 - lr: 5.0000e-06\n",
    "Epoch 14/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1166 - accuracy: 0.6000\n",
    "Epoch 14: val_loss improved from 1.09287 to 1.08694, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 497ms/step - loss: 1.1166 - accuracy: 0.6000 - val_loss: 1.0869 - val_accuracy: 0.6250 - lr: 5.0000e-06\n",
    "Epoch 15/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1221 - accuracy: 0.6028\n",
    "Epoch 15: val_loss improved from 1.08694 to 1.08551, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 488ms/step - loss: 1.1221 - accuracy: 0.6028 - val_loss: 1.0855 - val_accuracy: 0.6250 - lr: 5.0000e-06\n",
    "Epoch 16/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1106 - accuracy: 0.5989\n",
    "Epoch 16: val_loss improved from 1.08551 to 1.08033, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 488ms/step - loss: 1.1106 - accuracy: 0.5989 - val_loss: 1.0803 - val_accuracy: 0.6261 - lr: 5.0000e-06\n",
    "Epoch 17/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0777 - accuracy: 0.6136\n",
    "Epoch 17: val_loss improved from 1.08033 to 1.07700, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.0777 - accuracy: 0.6136 - val_loss: 1.0770 - val_accuracy: 0.6261 - lr: 5.0000e-06\n",
    "Epoch 18/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.1008 - accuracy: 0.6020\n",
    "Epoch 18: val_loss improved from 1.07700 to 1.07658, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.1008 - accuracy: 0.6020 - val_loss: 1.0766 - val_accuracy: 0.6216 - lr: 5.0000e-06\n",
    "Epoch 19/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0871 - accuracy: 0.6088\n",
    "Epoch 19: val_loss improved from 1.07658 to 1.07266, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 487ms/step - loss: 1.0871 - accuracy: 0.6088 - val_loss: 1.0727 - val_accuracy: 0.6159 - lr: 5.0000e-06\n",
    "Epoch 20/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0711 - accuracy: 0.6210\n",
    "Epoch 20: val_loss did not improve from 1.07266\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0711 - accuracy: 0.6210 - val_loss: 1.0735 - val_accuracy: 0.6205 - lr: 5.0000e-06\n",
    "Epoch 21/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0753 - accuracy: 0.6088\n",
    "Epoch 21: val_loss improved from 1.07266 to 1.07042, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 496ms/step - loss: 1.0753 - accuracy: 0.6088 - val_loss: 1.0704 - val_accuracy: 0.6205 - lr: 5.0000e-06\n",
    "Epoch 22/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.6108\n",
    "Epoch 22: val_loss improved from 1.07042 to 1.06485, saving model to best_model.keras\n",
    "110/110 [==============================] - 55s 494ms/step - loss: 1.0760 - accuracy: 0.6108 - val_loss: 1.0649 - val_accuracy: 0.6261 - lr: 2.5000e-06\n",
    "Epoch 23/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0718 - accuracy: 0.6108\n",
    "Epoch 23: val_loss improved from 1.06485 to 1.06448, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 493ms/step - loss: 1.0718 - accuracy: 0.6108 - val_loss: 1.0645 - val_accuracy: 0.6261 - lr: 1.2500e-06\n",
    "Epoch 24/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.6159\n",
    "Epoch 24: val_loss did not improve from 1.06448\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0816 - accuracy: 0.6159 - val_loss: 1.0645 - val_accuracy: 0.6250 - lr: 6.2500e-07\n",
    "Epoch 25/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0503 - accuracy: 0.6227\n",
    "Epoch 25: val_loss improved from 1.06448 to 1.06377, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 489ms/step - loss: 1.0503 - accuracy: 0.6227 - val_loss: 1.0638 - val_accuracy: 0.6261 - lr: 3.1250e-07\n",
    "Epoch 26/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0880 - accuracy: 0.6051\n",
    "Epoch 26: val_loss did not improve from 1.06377\n",
    "110/110 [==============================] - 52s 473ms/step - loss: 1.0880 - accuracy: 0.6051 - val_loss: 1.0640 - val_accuracy: 0.6250 - lr: 1.5625e-07\n",
    "Epoch 27/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0708 - accuracy: 0.6128\n",
    "Epoch 27: val_loss did not improve from 1.06377\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0708 - accuracy: 0.6128 - val_loss: 1.0641 - val_accuracy: 0.6250 - lr: 7.8125e-08\n",
    "Epoch 28/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0703 - accuracy: 0.6153\n",
    "Epoch 28: val_loss did not improve from 1.06377\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0703 - accuracy: 0.6153 - val_loss: 1.0642 - val_accuracy: 0.6239 - lr: 3.9062e-08\n",
    "Epoch 29/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0898 - accuracy: 0.5983\n",
    "Epoch 29: val_loss improved from 1.06377 to 1.06351, saving model to best_model.keras\n",
    "110/110 [==============================] - 54s 487ms/step - loss: 1.0898 - accuracy: 0.5983 - val_loss: 1.0635 - val_accuracy: 0.6261 - lr: 1.9531e-08\n",
    "Epoch 30/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0940 - accuracy: 0.5949\n",
    "Epoch 30: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 475ms/step - loss: 1.0940 - accuracy: 0.5949 - val_loss: 1.0636 - val_accuracy: 0.6261 - lr: 9.7656e-09\n",
    "Epoch 31/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0671 - accuracy: 0.6244\n",
    "Epoch 31: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0671 - accuracy: 0.6244 - val_loss: 1.0641 - val_accuracy: 0.6261 - lr: 4.8828e-09\n",
    "Epoch 32/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0611 - accuracy: 0.6179\n",
    "Epoch 32: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0611 - accuracy: 0.6179 - val_loss: 1.0644 - val_accuracy: 0.6227 - lr: 4.8828e-10\n",
    "Epoch 33/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0829 - accuracy: 0.6122\n",
    "Epoch 33: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0829 - accuracy: 0.6122 - val_loss: 1.0638 - val_accuracy: 0.6261 - lr: 4.8828e-11\n",
    "Epoch 34/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0885 - accuracy: 0.6037\n",
    "Epoch 34: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 476ms/step - loss: 1.0885 - accuracy: 0.6037 - val_loss: 1.0639 - val_accuracy: 0.6250 - lr: 4.8828e-12\n",
    "Epoch 35/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0676 - accuracy: 0.6108\n",
    "Epoch 35: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 475ms/step - loss: 1.0676 - accuracy: 0.6108 - val_loss: 1.0642 - val_accuracy: 0.6239 - lr: 4.8828e-13\n",
    "Epoch 36/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0833 - accuracy: 0.6071\n",
    "Epoch 36: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 476ms/step - loss: 1.0833 - accuracy: 0.6071 - val_loss: 1.0642 - val_accuracy: 0.6239 - lr: 4.8828e-14\n",
    "Epoch 37/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0671 - accuracy: 0.6091\n",
    "Epoch 37: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 475ms/step - loss: 1.0671 - accuracy: 0.6091 - val_loss: 1.0641 - val_accuracy: 0.6239 - lr: 4.8828e-15\n",
    "Epoch 38/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0618 - accuracy: 0.6202\n",
    "Epoch 38: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 53s 477ms/step - loss: 1.0618 - accuracy: 0.6202 - val_loss: 1.0640 - val_accuracy: 0.6239 - lr: 4.8828e-16\n",
    "Epoch 39/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.6193\n",
    "Epoch 39: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 53s 477ms/step - loss: 1.0760 - accuracy: 0.6193 - val_loss: 1.0640 - val_accuracy: 0.6250 - lr: 4.8828e-17\n",
    "Epoch 40/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0742 - accuracy: 0.6085\n",
    "Epoch 40: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0742 - accuracy: 0.6085 - val_loss: 1.0640 - val_accuracy: 0.6261 - lr: 4.8828e-18\n",
    "Epoch 41/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.6145\n",
    "Epoch 41: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0736 - accuracy: 0.6145 - val_loss: 1.0639 - val_accuracy: 0.6239 - lr: 4.8828e-19\n",
    "Epoch 42/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0590 - accuracy: 0.6156\n",
    "Epoch 42: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0590 - accuracy: 0.6156 - val_loss: 1.0639 - val_accuracy: 0.6273 - lr: 4.8828e-20\n",
    "Epoch 43/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0722 - accuracy: 0.6114\n",
    "Epoch 43: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0722 - accuracy: 0.6114 - val_loss: 1.0638 - val_accuracy: 0.6273 - lr: 4.8828e-21\n",
    "Epoch 44/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0961 - accuracy: 0.6017\n",
    "Epoch 44: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0961 - accuracy: 0.6017 - val_loss: 1.0642 - val_accuracy: 0.6250 - lr: 4.8828e-22\n",
    "Epoch 45/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0809 - accuracy: 0.6125\n",
    "Epoch 45: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 475ms/step - loss: 1.0809 - accuracy: 0.6125 - val_loss: 1.0641 - val_accuracy: 0.6239 - lr: 4.8828e-23\n",
    "Epoch 46/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0769 - accuracy: 0.6170\n",
    "Epoch 46: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 475ms/step - loss: 1.0769 - accuracy: 0.6170 - val_loss: 1.0645 - val_accuracy: 0.6239 - lr: 4.8828e-24\n",
    "Epoch 47/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.6125\n",
    "Epoch 47: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0732 - accuracy: 0.6125 - val_loss: 1.0640 - val_accuracy: 0.6261 - lr: 4.8828e-25\n",
    "Epoch 48/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.6176\n",
    "Epoch 48: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0904 - accuracy: 0.6176 - val_loss: 1.0642 - val_accuracy: 0.6250 - lr: 4.8828e-26\n",
    "Epoch 49/100\n",
    "110/110 [==============================] - ETA: 0s - loss: 1.0692 - accuracy: 0.6151Restoring model weights from the end of the best epoch: 29.\n",
    "\n",
    "Epoch 49: val_loss did not improve from 1.06351\n",
    "110/110 [==============================] - 52s 474ms/step - loss: 1.0692 - accuracy: 0.6151 - val_loss: 1.0646 - val_accuracy: 0.6239 - lr: 4.8828e-27\n",
    "Epoch 49: early stopping\n",
    "35/35 [==============================] - 9s 241ms/step\n",
    "\n",
    "Классификационный отчет:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Beige       0.61      0.71      0.66       100\n",
    "       Black       0.49      0.57      0.53       100\n",
    "        Blue       0.58      0.74      0.65       100\n",
    "       Brown       0.53      0.49      0.51       100\n",
    "       Green       0.53      0.28      0.37       100\n",
    "        Grey       0.39      0.28      0.33       100\n",
    "      Orange       0.58      0.61      0.60       100\n",
    "         Red       0.71      0.65      0.68       100\n",
    "      Silver       0.59      0.62      0.60       100\n",
    "       White       0.79      0.85      0.82       100\n",
    "      Yellow       0.76      0.87      0.81       100\n",
    "\n",
    "    accuracy                           0.70       910\n",
    "   macro avg       0.71      0.71      0.70       910\n",
    "weighted avg       0.72      0.70      0.71       910\n",
    "\n",
    "\n",
    "F1 Macro: 0.7038\n",
    "\"\"\""
   ],
   "id": "52420270d9706527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Итоги по второму эксперименту\n",
    "### F1 Macro: 0.7038\n",
    "![Графики обучения модели](plots/graph_pretrained_blocked.jpg)\n",
    "![Матрица ошибок](plots/matrix_custom.jpg)\n"
   ],
   "id": "d1b7e01bed3af2f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Выводы\n",
    "## Сравнение результатов\n",
    "- Результат первого эксперимента = 0.7454\n",
    "- Результат второго эксперимента = 0.7038\n",
    "\n",
    "Эксперимент показал, предобученная модель показала себя хуже, чем модель с нуля.\n",
    "\n",
    "Это может быть связано с тем, что предобученная модель обучалась на других данных и не смогла нормально адаптироваться к новым данным.\n",
    "\n",
    "## Подход к задаче\n",
    "Чтобы добиться максимально возможного качества модели прошлось отнормировать данные (привести все возможные цвета к одному значению). Если ограничить данные просто количеством классов с макимальными цветами (например топ 6 или меньше) точность увеличивается и переваливает за 80% (но это не совсем то, что требуется в задаче).\n",
    "\n",
    "## Оценка качества моделей\n",
    "Качество моделей на среднем уровне\n",
    "\n",
    "Это может быть связано с тем, что данные в выборке распределены неравномерно:\n",
    "- Некоторые цвета (белый, серый и серебристый) можно спутать из-за условий на картинке\n",
    "- Аугментация данных не сильно помогла улучшить качество модели\n",
    "- качество картинок может быть низким\n",
    "\n",
    "Кроме того это может быть связано с тем что отсутствует разметка данных на промежуточных этапах"
   ],
   "id": "fe9881a58ff82570"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b45bb7145d2c98ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
